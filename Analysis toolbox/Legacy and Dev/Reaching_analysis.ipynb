{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d363a9d3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# ReachOut - reaching movement kinematics analysis\n",
    "___\n",
    "\n",
    "## This is the Notebook containing all the steps to prepare the video for analysis with DEEPLABCUT and Anipose and then segment and vizualize the trajectories:\n",
    "\n",
    "### 0. Cut the video from the FLIR camera into three parts - three different views of the reach\n",
    "We are recording the videos of the reaching task using the single high speed widefield FLIR camera and multiple mirrors, an then cut the single video into parts corresponding to the Leftside view (CamA), Center view (CamB), Rightside view (CamC). This eliminates the need for synchronisation of different videos, but adds this additional step before we can analyze our videos with DLC. \n",
    "As a result of this step you should get three video files with the names -camA, -camB, -camC \n",
    "\n",
    "### 1. Analyze the video with DeepLabCut\n",
    "This step is done using the open-source DEEPLABCUT and Anipose packages, so the tutorial and more info on them can be found on GitHub\n",
    "__[https://github.com/DeepLabCut](DEEPLABCUT)__\n",
    "\n",
    "It should be further noted that despite the Anipose documentation advice to use single DLC network for all views we found that better results are achieved with two separate networks trained on the side(1) and front(2) view. That means we start DLC analysis for our videos twice - first time for all the CamA/C videos, and the second time for CamB.\n",
    "As a result we get two separate sets of _*.csv *.h5_ files. For the next step they should be copied together to the Anipose folder\n",
    "\n",
    "### 2. Triangulate the 2d trajectories with Anipose \n",
    "This step is done using the open-source Anipose package, so the tutorial and more info on them can be found on GitHub\n",
    "__[https://github.com/lambdaloop/anipose](ANIPOSE)__\n",
    "\n",
    "To get use of Anipose triangulation we manually put the resulting .csv and .h5 files into anipose project folder _*/Anipose project/pose-2d before_ before starting the triangulation command _anipose triangulate_\n",
    "\n",
    "As a result of this step you should get the single _*.csv_ file for each session, containing the x,y,z coordinates that we will use further  \n",
    "\n",
    "### 3. Open the csv file, scroll through the trajectories and manually choose the fragments of the trajectory with the reaches for analysis\n",
    "\n",
    "As a result of this step you will have the _*.h5_ file with the fragments of the trajectory that you've chosen. Open this file to review the trajectories.  \n",
    "\n",
    "### 4. Open the h5 file and the corresponding video to review and classify the trajectories\n",
    "\n",
    "For this script to work point it to the _*.h5_ file you want to analyze (file) and the video corresponding to the same session (file2)\n",
    "As a result of this step you will have the _*.h5_ file which is used in the vizualisation steps 5. and 6.\n",
    "\n",
    "### 5. Open the scalars.h5 file and plot all the trajectories of the chosen category\n",
    "\n",
    "### 6. Open the scalars.h5 file and make violin plots for the chosen parameters and reach categories  \n",
    "  \n",
    "  \n",
    "  ***\n",
    "  ___\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5c4e4",
   "metadata": {},
   "source": [
    "# Analysis part\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1160162",
   "metadata": {},
   "source": [
    "#### 0. Cut the video from the FLIR camera \n",
    "Note that we have two scripts for two use cases. If you need to process just a single file, type _from video_split import video_split_ and if you want to process all files in a certain folder type _from video_split_folder import video_split_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5752cc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_13_50_22_calibrationis split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_13_56_22_ncb1042is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_14_17_44_calibrationis split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_14_22_24_ncb1043is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_14_27_47_ncb1042is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_14_29_27_calibrationis split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_14_37_05_ncb1042is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_14_42_06_calibrationis split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_14_43_57_ncb1042is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_14_44_04_ncb1045is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_14_52_48_ncb1043is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_15_01_31_ncb1043is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_15_15_01_ncb1045is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_15_15_12_ncb1043is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_15_39_53_ncb1046is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_15_40_27_ncb1045is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_15_46_46_ncb1046is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_16_13_03_ncb1047is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_16_14_08_ncb1044is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_16_14_26_ncb1047is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_16_21_10_calibrationis split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_16_22_43_ncb1042is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_16_26_18_ncb1045is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_16_31_30_ncb1046is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_16_36_34_ncb1051is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_16_43_01_nbc1043is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_16_58_28_ncb1047is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_16_58_44_ncb1046is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_16_59_33_ncb1045is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_17_00_24_ncb1051is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_17_07_29_ncb1052is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_17_23_15_ncb1047is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_17_23_35_ncb1046is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_17_23_56_ncb1051is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_17_25_32_ncb1052is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_17_40_38_ncb1047is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_17_47_10_ncb1051is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_17_51_21_ncb1052is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_17_58_49_ncb1051is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_18_12_21_ncb1052is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7/mj_18_17_57_ncb1052is split to camA,B,C\n",
      "Done! Your videos are there: //pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/Trial Batch 7/training1-7\n"
     ]
    }
   ],
   "source": [
    "# if you want to do bulk analysis on all the files in a folder - uncomment the second line and comment the first one \n",
    "#from video_split import video_split           #for single file\n",
    "from video_split_folder import video_split    #for multiple files\n",
    "\n",
    "video_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcace58",
   "metadata": {},
   "source": [
    "#### 1.  Analyze the video with DeepLabCut\n",
    "The following line should open DLC interface. Open the project containing your trained network, open the analyze video layout and select the videos you want to track (supposedly, either all camA, camB, or camC videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3dfdfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import deeplabcut\n",
    "deeplabcut.launch_dlc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d199b5",
   "metadata": {},
   "source": [
    "#### 2. Triangulate the 2d trajectories with Anipose \n",
    "By now this is done outside of the jupyter notebook, because you need to perform several steps, not including the Anipose calibration: <ol> <li>-  manually make an Anipose directory </li>  <li>-  rename the files after DLC analysis, leaving just the *camX* in the end </li> <li>- transfer the .csv and .h5 files with the tracking results to the Anipose directory </li> <li>- *cd* to this directory and </li> <li>- run from the command prompt *anipose triangulate* </li> </ol>\n",
    "\n",
    "We also provide the 'simpler' way of doing it within the Jupiter Notebook. The following two cells allow to perform the Camera Calibration and Camera Triangulation with the help of AniposeLib. However, you still need to manually make videos for camera calibration and point to them in the first step. The output of this step will be the calibration.toml file with camera extrinsics and intrinsics. The path to this file as well as the path two videos in need for triangulation should be pointed in the second step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc06cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALIBRATING THE CAMERAS\n",
    "\n",
    "import numpy as np\n",
    "from aniposelib.boards import CharucoBoard, Checkerboard\n",
    "from aniposelib.cameras import Camera, CameraGroup\n",
    "from aniposelib.utils import load_pose2d_fnames\n",
    "\n",
    "# modify the following lines to match your project\n",
    "vidnames = [['Anipose Project/2019-08-02/calibration/mj_14_55_41_calibration_right-camA.mp4'], \n",
    "            ['Anipose Project/2019-08-02/calibration/mj_14_55_41_calibration_right-camB.mp4']]\n",
    "\n",
    "cam_names = ['A', 'B']\n",
    "\n",
    "n_cams = len(vidnames)\n",
    "\n",
    "board = Checkerboard(4, 4,\n",
    "                     square_length=5,  manually_verify=True) # here, in mm but any unit works\n",
    "                     \n",
    "\n",
    "\n",
    "# the videos provided are fisheye, so we need the fisheye option\n",
    "cgroup = CameraGroup.from_names(cam_names, fisheye=False)\n",
    "\n",
    "# this will take about 15 minutes (mostly due to detection)\n",
    "# it will detect the charuco board in the videos,\n",
    "# then calibrate the cameras based on the detections, using iterative bundle adjustment\n",
    "cgroup.calibrate_videos(vidnames, board)\n",
    "\n",
    "# if you need to save and load\n",
    "# example saving and loading for later\n",
    "cgroup.dump('calibration.toml')\n",
    "\n",
    "## example of loading calibration from a file\n",
    "## you can also load the provided file if you don't want to wait 15 minutes\n",
    "#cgroup = CameraGroup.load('calibration.toml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRIANGULATING THE ACTUAL COORDINATES WITH THE USE OF CALIBRATION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from aniposelib.boards import CharucoBoard, Checkerboard\n",
    "from aniposelib.cameras import Camera, CameraGroup\n",
    "from aniposelib.utils import load_pose2d_fnames\n",
    "from coord_correct import correct_coordinate_frame, get_median, ortho, proj\n",
    "\n",
    "#modify the following lines to match your project\n",
    "output_fname = '12ncb1025.csv'\n",
    "cgroup = CameraGroup.load('calibration.toml')\n",
    "score_threshold = 0.1\n",
    "\n",
    "# loading data from DLC\n",
    "fname_dict = {\n",
    "    'B': 'updated_mj_16_25_05_ncb1025-camB.h5',\n",
    "    'C': 'updated_mj_16_25_05_ncb1025-camC.h5'\n",
    "}\n",
    "\n",
    "d = load_pose2d_fnames(fname_dict, cam_names=cgroup.get_names())\n",
    "\n",
    "n_cams, n_points, n_joints, _ = d['points'].shape\n",
    "points = d['points']\n",
    "scores = d['scores']\n",
    "bodyparts = d['bodyparts']\n",
    "\n",
    "# remove points that are below threshold\n",
    "points[scores < score_threshold] = np.nan\n",
    "\n",
    "points_flat = points.reshape(n_cams, -1, 2)\n",
    "scores_flat = scores.reshape(n_cams, -1)\n",
    "\n",
    "p3ds_flat = cgroup.triangulate(points_flat, progress=True)\n",
    "reprojerr_flat = cgroup.reprojection_error(p3ds_flat, points_flat, mean=True)\n",
    "\n",
    "p3ds = p3ds_flat.reshape(n_points, n_joints, 3)\n",
    "reprojerr = reprojerr_flat.reshape(n_points, n_joints)\n",
    "\n",
    "\n",
    "# This part of the code (if uncommented) is correcting the coordinate frame to match the axes entered and takes\n",
    "# the reference point as zero, substracting the median of the corresponding column from all points tracked\n",
    "\n",
    "axes = [\n",
    "    [\"x\", \"starting_point\", \"pellet\"],\n",
    "    [\"y\", \"starting_point\", \"border\"]\n",
    "]\n",
    "\n",
    "reference_point = \"pellet\"\n",
    "\n",
    "config=pd.DataFrame(columns=['triangulation'], index=['axes','reference_point'])\n",
    "config['triangulation']['axes'] = [\n",
    "    [\"x\", \"starting_point\", \"pellet\"],\n",
    "    [\"y\", \"starting_point\", \"border\"]\n",
    "]\n",
    "\n",
    "config['triangulation']['reference_point'] = \"pellet\"\n",
    "\n",
    "###\n",
    "\n",
    "if 'reference_point' in config['triangulation'] and 'axes' in config['triangulation']:\n",
    "    all_points_3d_adj, M, center = correct_coordinate_frame(config, p3ds, bodyparts)\n",
    "else:\n",
    "    all_points_3d_adj = p3ds\n",
    "    M = np.identity(3)\n",
    "    center = np.zeros(3)\n",
    "    \n",
    "# concatenating everything in the dataframe and saving to csv   \n",
    "good_points = ~np.isnan(points[:, :, :, 0])\n",
    "num_cams = np.sum(good_points, axis=0).astype('float')\n",
    "scores[~good_points] = 2\n",
    "scores_3d = np.min(scores, axis=0)\n",
    "\n",
    "\n",
    "dout = pd.DataFrame()\n",
    "for bp_num, bp in enumerate(bodyparts):\n",
    "    for ax_num, axis in enumerate(['x','y','z']):\n",
    "        dout[bp + '_' + axis] = all_points_3d_adj[:, bp_num, ax_num]\n",
    "    dout[bp + '_error'] = reprojerr[:, bp_num]\n",
    "    dout[bp + '_ncams'] = num_cams[:, bp_num]\n",
    "    dout[bp + '_score'] = scores_3d[:, bp_num]\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        dout['M_{}{}'.format(i, j)] = M[i, j]\n",
    "\n",
    "for i in range(3):\n",
    "    dout['center_{}'.format(i)] = center[i]\n",
    "\n",
    "dout['fnum'] = np.arange(n_points)\n",
    "\n",
    "dout.to_csv(output_fname, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c6db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASIC VERIFICATION OF THE COORDINATES\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.figure(figsize=(9.4, 6))\n",
    "axes1 = fig.add_subplot(111)\n",
    "axes2 = axes1.twinx()\n",
    "#axes1.plot(p3ds[:, 0, 0])\n",
    "axes1.plot(p3ds[:, 0, 1])\n",
    "#axes2.plot(p3ds[:, 0, 2])\n",
    "plt.xlabel(\"Time (frames)\")\n",
    "plt.ylabel(\"Coordinate (mm)\")\n",
    "plt.title(\"x, y, z coordinates of {}\".format(bodyparts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6cf3c",
   "metadata": {},
   "source": [
    "#### 3. Open the csv file, scroll through the trajectories\n",
    "\n",
    "The following program snippet allows you to open the csv file with Anipose output and scroll through the traejctories, visualizing them as a 3d plot + 2 projections: to X and Y axis. \n",
    "**Scroll** by moving the slider at the very bottom. **The 3d plot can be rotated** by howering on top of it with the mouse and holding left button.\n",
    "Also when you move your mouse to the left lower X-projection plot and hold the left button you can choose the smaller part of the trajectory to visualize with a **span selector**. \n",
    "If after close inspection you find this part of the trajectory useful for further analysis (it looks like a valid reach or any other action you are after), you should press the left **Save button** to add it to the resulting output table.\n",
    "The **Save_all button** on the right saves the resulting table with all the parameters as a *.h5* file for further analysis.\n",
    "\n",
    "The parameters calculated and added to the table at this step:\n",
    "\n",
    "Taken from original csv file\n",
    "* paw_x\n",
    "* paw_y\n",
    "* paw_z\n",
    "* paw_error\n",
    "* paw_score\n",
    "\n",
    "Calculated and added as new columns\n",
    "* dX\n",
    "* dY\n",
    "* dZ\n",
    "* dE\n",
    "* time\n",
    "* time_diff\n",
    "* velocity\n",
    "* acceleration\n",
    "* jerk\n",
    "\n",
    "\n",
    "The resulting *.h5 file contains two keys:\n",
    "* data - pd dataframe containing the updated data table loaded from csv\n",
    "* reach_index - list of arrays containing start and stop frame for all the reaches chosen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ce6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# for more dynamic programs like this one we are switching to the interactive matplotlib backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b52a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83f4c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tracking_split\n",
    "tracking = tracking_split.TrackingViewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd97d4",
   "metadata": {},
   "source": [
    "#### 4. Open the h5 file and the corresponding video to review and classify the trajectories\n",
    "The following program opens the file with all the trajectories saved on the previous step, along with the video file **(the path to both should be typed in the following cell)**\n",
    "Left dropdown list shows all the reaches with the corresponding frames in a video. As soon as you choose one the popup window will show the corresponding part of the video. To **close the window** type Q on the keyboard. After inspecting the video an accompanying plots **mark the reach** with one of the categories on the right. By default all the reaches are marked as Missed or the last chosen option in the **right list**. As soon as you click on one of the options, the label for the current reach gets updated. When you finish **click Save** to write down the changes to the file. The file is saved with the same name as the input file with added *_scalar.h5* suffix and used in all the visualization steps. \n",
    "\n",
    "The parameters calculated for every reach at this step, effectively describing every reach:\n",
    "\n",
    "* time_difference\n",
    "* dX\n",
    "* dY\n",
    "* dZ\n",
    "* dE\n",
    "* maxX\n",
    "* minX\n",
    "* min_abs_X\n",
    "* maxY\n",
    "* minY\n",
    "* min_abs_Y\n",
    "* maxZ\n",
    "* minZ\n",
    "* min_abs_Z\n",
    "* mean_velocity\n",
    "* mean_acceleration\n",
    "* mean_jerk\n",
    "* max_velocity\n",
    "* max_acceleration\n",
    "* max_jerk\n",
    "* max_velocity_position\n",
    "* maxX_position\n",
    "* minY_position\n",
    "* maxZ_position\n",
    "\n",
    "The resulting *_scalar.h5* file contains four keys:\n",
    "\n",
    "* cleared_data - pd dataframe, the copy of raw data table with only chosen reaches left, all other timepoints as NaNs \n",
    "* reaches - pd dataframe with start and stop position for every reach\n",
    "* mean - pd dataframe with parameters as columns and reaches as indexes, includes the group column with reach type\n",
    "* std - similar to mean, pd dataframe with parameters (STDs) as columns and reaches as indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5876024",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# for more static programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3640de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import viewer\n",
    "directory = 'C:/Users/daniil.berezhnoi.VAI/Videos/3d plot - manual assembly/batch analysis/'\n",
    "filename = 'mj_16_25_05_ncb1025-camB.mp4'\n",
    "filename2 = 'updated_12ncb1025.h5'\n",
    "file = directory+filename\n",
    "file2 = directory+filename2\n",
    "\n",
    "view = viewer.ReachesViewer(file, file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b632c6",
   "metadata": {},
   "source": [
    "# Visualization part\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df31152",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fdd06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook  \n",
    "# for interactive programs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af0ebba",
   "metadata": {},
   "source": [
    "#### 5. Open the scalars.h5 file and plot all the trajectories of the chosen category\n",
    "This snippet shows all reaches in the chosen category as Timeseries for visual inspection and analysis. From the lists you can choose the category of reaches and the type of data to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba9fe3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import reach_view\n",
    "viewer = reach_view.TimeSeriesViewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb0cf6b",
   "metadata": {},
   "source": [
    "#### 6. Open the scalars.h5 file and make violin plots for the chosen parameters and reach categories\n",
    "This snippet shows all the calculated parameters for all reaches in the current session in the form of violin plots. \n",
    "The categories to show can be chosen from the left list **(Shift/Ctrl+Click for multiple choise)** and parameters - from the right list.\n",
    "**Left plot** represents mean for the chosen parameter for every reach (data points).\n",
    "**Right plot** represents variance for the chosen parameter for every reach (data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3661400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scalar_view\n",
    "viewer = scalar_view.InteractiveScalarViewer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DEEPLABCUT] *",
   "language": "python",
   "name": "conda-env-DEEPLABCUT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
