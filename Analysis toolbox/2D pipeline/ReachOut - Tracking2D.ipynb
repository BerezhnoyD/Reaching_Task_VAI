{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d363a9d3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# ReachOut - reaching movement kinematics analysis - Part1 (Tracking)\n",
    "___\n",
    "\n",
    "## This Notebook contains the scripts to prepare the video for analysis with DEEPLABCUT and Anipose and generate the 3d trajectories:\n",
    "\n",
    "### 0. Cut the video from the FLIR camera into three parts - three different views of the reach\n",
    "We are recording the videos of the reaching task using the single high speed widefield FLIR camera and multiple mirrors, an then cut the single video into parts corresponding to the Leftside view (CamA), Center view (CamB), Rightside view (CamC). This eliminates the need for synchronisation of different videos, but adds this additional step before we can analyze our videos with DLC. \n",
    "As a result of this step you should get three video files with the names -camA, -camB, -camC \n",
    "\n",
    "### 1. Analyze the video with DeepLabCut\n",
    "This step is done using the open-source DEEPLABCUT and Anipose packages, so the tutorial and more info on them can be found on GitHub\n",
    "__[https://github.com/DeepLabCut](DEEPLABCUT)__\n",
    "\n",
    "As a result we get  _*.csv *.h5_ files. With the csv file you can proceed to the next notebook\n",
    "\n",
    "  \n",
    "  ***\n",
    "  ___\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5c4e4",
   "metadata": {},
   "source": [
    "# Tracking and triagulation part\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1160162",
   "metadata": {},
   "source": [
    "#### 0. Cut the video from the FLIR camera \n",
    "Note that we have two scripts for two use cases. If you need to process just a single file, type _from video_split import video_split_ and if you want to process all files in a certain folder type _from video_split_folder import video_split_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5752cc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Your video//pn.vai.org/projects_primary/chu/Daniil Berezhnoi/Behaviour/ReachingTask/Reaching+IT neurons Ca/from Batch2/10_04-Recording2/mj_15_55_02_calibrationis split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Daniil Berezhnoi/Behaviour/ReachingTask/Reaching+IT neurons Ca/from Batch2/10_04-Recording2/mj_16_06_48_ncb1841is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Daniil Berezhnoi/Behaviour/ReachingTask/Reaching+IT neurons Ca/from Batch2/10_04-Recording2/mj_16_08_27_ncb1841is split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Daniil Berezhnoi/Behaviour/ReachingTask/Reaching+IT neurons Ca/from Batch2/10_04-Recording2/mj_16_43_09_ncb1847.mp4_fixedis split to camA,B,C\n",
      "Done! Your video//pn.vai.org/projects_primary/chu/Daniil Berezhnoi/Behaviour/ReachingTask/Reaching+IT neurons Ca/from Batch2/10_04-Recording2/mj_17_15_10_ncb1855.mp4_fixedis split to camA,B,C\n",
      "Done! Your videos are there: //pn.vai.org/projects_primary/chu/Daniil Berezhnoi/Behaviour/ReachingTask/Reaching+IT neurons Ca/from Batch2/10_04-Recording2\n"
     ]
    }
   ],
   "source": [
    "# if you want to do bulk analysis on all the files in a folder - uncomment the second line and comment the first one \n",
    "#from video_split import video_split           #for single file uncomment this line\n",
    "from video_split_folder import video_split     #for multiple files \n",
    "\n",
    "video_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcace58",
   "metadata": {},
   "source": [
    "#### 1.  Analyze the video with DeepLabCut\n",
    "The following line should open DLC interface. Open the project containing your trained network, open the analyze video layout and select the videos you want to track (supposedly, either all camA, camB, or camC videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3dfdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-1000000 for model \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Python programs\\July Networks 2023\\LeftHanded\\Center_mirror-Daniil-2022-09-02\\dlc-models\\iteration-1\\Center_mirrorSep2-trainset95shuffle2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "C:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tf_slim\\layers\\layers.py:684: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  outputs = layer.apply(inputs, training=is_training)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_01_16_ncb1014-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_01_16_ncb1014-camB.mp4\n",
      "Duration of video [s]:  1952.88 , recorded with  25.0 fps!\n",
      "Overall # of frames:  48822  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49288it [11:05, 74.06it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional...\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_04_17_ncb1042-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_04_17_ncb1042-camB.mp4\n",
      "Duration of video [s]:  1872.48 , recorded with  25.0 fps!\n",
      "Overall # of frames:  46812  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47268it [10:27, 75.28it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional...\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_05_34_ncb1130-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_05_34_ncb1130-camB.mp4\n",
      "Duration of video [s]:  1857.64 , recorded with  25.0 fps!\n",
      "Overall # of frames:  46441  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46864it [10:23, 75.19it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional...\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_30_43_ncb1015-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_30_43_ncb1015-camB.mp4\n",
      "Duration of video [s]:  1513.4 , recorded with  25.0 fps!\n",
      "Overall # of frames:  37835  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38178it [08:28, 75.06it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional...\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_02_26_ncb1016-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_02_26_ncb1016-camB.mp4\n",
      "Duration of video [s]:  1778.56 , recorded with  25.0 fps!\n",
      "Overall # of frames:  44464  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44844it [09:59, 74.81it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional...\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_25_05_ncb1025-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_25_05_ncb1025-camB.mp4\n",
      "Duration of video [s]:  1613.44 , recorded with  25.0 fps!\n",
      "Overall # of frames:  40336  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40703it [09:02, 75.05it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional...\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_47_49_ncb1153-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_47_49_ncb1153-camB.mp4\n",
      "Duration of video [s]:  2018.4 , recorded with  25.0 fps!\n",
      "Overall # of frames:  50460  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50904it [11:16, 75.30it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional...\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_53_37_ncb1046-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_53_37_ncb1046-camB.mp4\n",
      "Duration of video [s]:  2010.04 , recorded with  25.0 fps!\n",
      "Overall # of frames:  50251  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50702it [11:12, 75.38it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional...\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_01_16_ncb1014-camB.mp4\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_04_17_ncb1042-camB.mp4\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_05_34_ncb1130-camB.mp4\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_30_43_ncb1015-camB.mp4\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_02_26_ncb1016-camB.mp4\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_25_05_ncb1025-camB.mp4\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_47_49_ncb1153-camB.mp4\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_53_37_ncb1046-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_01_16_ncb1014-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_04_17_ncb1042-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_05_34_ncb1130-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_30_43_ncb1015-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_02_26_ncb1016-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_25_05_ncb1025-camB.mp4 and data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\utils\\plotting.py:54: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig1 = plt.figure(figsize=(8, 6))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_47_49_ncb1153-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_53_37_ncb1046-camB.mp4 and data.\n",
      "Plots created! Please check the directory \"plot-poses\" within the video directory\n",
      "Using snapshot-1000000 for model \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Python programs\\July Networks 2023\\RightHanded\\Center_Mirror-Daniil-2023-07-24\\dlc-models\\iteration-0\\Center_MirrorJul24-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "C:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tf_slim\\layers\\layers.py:684: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  outputs = layer.apply(inputs, training=is_training)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_14_57_17_ncb1013-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_14_57_17_ncb1013-camB.mp4\n",
      "Duration of video [s]:  1358.64 , recorded with  25.0 fps!\n",
      "Overall # of frames:  33966  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34239it [07:38, 74.60it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional...\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_51_51_ncb1131-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_51_51_ncb1131-camB.mp4\n",
      "Duration of video [s]:  2050.8 , recorded with  25.0 fps!\n",
      "Overall # of frames:  51270  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51712it [11:30, 74.85it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional...\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_14_53_ncb1155-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_14_53_ncb1155-camB.mp4\n",
      "Duration of video [s]:  2123.56 , recorded with  25.0 fps!\n",
      "Overall # of frames:  53089  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53530it [11:57, 74.56it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional...\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_48_46_ncb1404-R-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_48_46_ncb1404-R-camB.mp4\n",
      "Duration of video [s]:  2816.0 , recorded with  25.0 fps!\n",
      "Overall # of frames:  70400  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71104it [15:47, 75.02it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional...\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_17_13_44_ncb1405-R-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_17_13_44_ncb1405-R-camB.mp4\n",
      "Duration of video [s]:  3017.32 , recorded with  25.0 fps!\n",
      "Overall # of frames:  75433  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76154it [16:51, 75.26it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional...\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_17_18_10_ncb1019-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_17_18_10_ncb1019-camB.mp4\n",
      "Duration of video [s]:  1515.16 , recorded with  25.0 fps!\n",
      "Overall # of frames:  37879  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38178it [08:28, 75.05it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional...\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_14_57_17_ncb1013-camB.mp4\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_51_51_ncb1131-camB.mp4\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_14_53_ncb1155-camB.mp4\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_48_46_ncb1404-R-camB.mp4\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_17_13_44_ncb1405-R-camB.mp4\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_17_18_10_ncb1019-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_14_57_17_ncb1013-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_15_51_51_ncb1131-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_14_53_ncb1155-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_16_48_46_ncb1404-R-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_17_13_44_ncb1405-R-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Hiba Douja Chehade\\Behavior\\Reach task\\10wo training\\10wo timepoint video analysis\\Batches from 2022\\additional\\mj_17_18_10_ncb1019-camB.mp4 and data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\utils\\plotting.py:54: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig1 = plt.figure(figsize=(8, 6))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots created! Please check the directory \"plot-poses\" within the video directory\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "deeplabcut.launch_dlc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c807580",
   "metadata": {},
   "source": [
    "The following line can be used to open, look through and correct the DLC output files with tracking using the standard pandas functions (optional). We're using this function to add reference points for **triangulation** step: pellet, starting_point and border of the training box. Coordinates for this points in the video are entered manually and used during **triangulation** to construct primary axes: forward (X), sideward (Y) and upward (Z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88d9c149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>scorer</th>\n",
       "      <th colspan=\"21\" halign=\"left\">DLC_resnet50_Center_MirrorJul24shuffle1_1000000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bodyparts</th>\n",
       "      <th colspan=\"3\" halign=\"left\">finger1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">finger2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">finger3</th>\n",
       "      <th>finger4</th>\n",
       "      <th>...</th>\n",
       "      <th>wrist_joint</th>\n",
       "      <th colspan=\"3\" halign=\"left\">pellet</th>\n",
       "      <th colspan=\"3\" halign=\"left\">starting_point</th>\n",
       "      <th colspan=\"3\" halign=\"left\">border</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coords</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>...</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>408.760010</td>\n",
       "      <td>297.913818</td>\n",
       "      <td>3.820793e-04</td>\n",
       "      <td>412.238800</td>\n",
       "      <td>292.275360</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>408.195679</td>\n",
       "      <td>294.789429</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>407.666809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>427</td>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>426</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>409.011444</td>\n",
       "      <td>297.913818</td>\n",
       "      <td>1.789902e-04</td>\n",
       "      <td>412.444244</td>\n",
       "      <td>292.275360</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>408.195679</td>\n",
       "      <td>295.485596</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>407.991821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>427</td>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>426</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>421.307495</td>\n",
       "      <td>297.913818</td>\n",
       "      <td>2.675425e-05</td>\n",
       "      <td>413.447266</td>\n",
       "      <td>292.275360</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>412.044434</td>\n",
       "      <td>295.485596</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>409.990204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>427</td>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>426</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>421.307495</td>\n",
       "      <td>294.283752</td>\n",
       "      <td>3.876089e-05</td>\n",
       "      <td>413.447266</td>\n",
       "      <td>292.275360</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>412.044434</td>\n",
       "      <td>295.485596</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>409.990204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>427</td>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>426</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>424.647125</td>\n",
       "      <td>293.108521</td>\n",
       "      <td>2.047014e-05</td>\n",
       "      <td>413.447266</td>\n",
       "      <td>292.275360</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.690141</td>\n",
       "      <td>346.193329</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>409.990204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>427</td>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>426</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>421.307495</td>\n",
       "      <td>293.108521</td>\n",
       "      <td>3.898310e-07</td>\n",
       "      <td>0.722302</td>\n",
       "      <td>348.496216</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.651844</td>\n",
       "      <td>346.206726</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>408.523132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>427</td>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>426</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>418.076874</td>\n",
       "      <td>292.656616</td>\n",
       "      <td>4.947227e-07</td>\n",
       "      <td>0.722302</td>\n",
       "      <td>348.496216</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.690141</td>\n",
       "      <td>346.193329</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>406.798920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>427</td>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>426</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>397.114838</td>\n",
       "      <td>293.108521</td>\n",
       "      <td>3.419030e-07</td>\n",
       "      <td>0.722302</td>\n",
       "      <td>348.511261</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.690141</td>\n",
       "      <td>346.205170</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>406.798920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>427</td>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>426</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.150483</td>\n",
       "      <td>314.762909</td>\n",
       "      <td>2.175456e-06</td>\n",
       "      <td>0.794935</td>\n",
       "      <td>348.519226</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.713504</td>\n",
       "      <td>346.200562</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>408.165070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>427</td>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>426</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-2.366575</td>\n",
       "      <td>314.762909</td>\n",
       "      <td>3.418058e-07</td>\n",
       "      <td>0.811944</td>\n",
       "      <td>348.546295</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.716809</td>\n",
       "      <td>346.205170</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>408.165070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>427</td>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>426</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "scorer    DLC_resnet50_Center_MirrorJul24shuffle1_1000000              \\\n",
       "bodyparts                                         finger1               \n",
       "coords                                                  x           y   \n",
       "0                                              408.760010  297.913818   \n",
       "1                                              409.011444  297.913818   \n",
       "2                                              421.307495  297.913818   \n",
       "3                                              421.307495  294.283752   \n",
       "4                                              424.647125  293.108521   \n",
       "5                                              421.307495  293.108521   \n",
       "6                                              418.076874  292.656616   \n",
       "7                                              397.114838  293.108521   \n",
       "8                                               -0.150483  314.762909   \n",
       "9                                               -2.366575  314.762909   \n",
       "\n",
       "scorer                                                                  \\\n",
       "bodyparts                   finger2                            finger3   \n",
       "coords       likelihood           x           y likelihood           x   \n",
       "0          3.820793e-04  412.238800  292.275360   0.000080  408.195679   \n",
       "1          1.789902e-04  412.444244  292.275360   0.000055  408.195679   \n",
       "2          2.675425e-05  413.447266  292.275360   0.000014  412.044434   \n",
       "3          3.876089e-05  413.447266  292.275360   0.000005  412.044434   \n",
       "4          2.047014e-05  413.447266  292.275360   0.000005    0.690141   \n",
       "5          3.898310e-07    0.722302  348.496216   0.000002    0.651844   \n",
       "6          4.947227e-07    0.722302  348.496216   0.000002    0.690141   \n",
       "7          3.419030e-07    0.722302  348.511261   0.000002    0.690141   \n",
       "8          2.175456e-06    0.794935  348.519226   0.000006    0.713504   \n",
       "9          3.418058e-07    0.811944  348.546295   0.000002    0.716809   \n",
       "\n",
       "scorer                                        ...                          \\\n",
       "bodyparts                            finger4  ... wrist_joint pellet        \n",
       "coords              y likelihood           x  ...  likelihood      x    y   \n",
       "0          294.789429   0.000012  407.666809  ...    0.000976    427  342   \n",
       "1          295.485596   0.000008  407.991821  ...    0.001298    427  342   \n",
       "2          295.485596   0.000008  409.990204  ...    0.000174    427  342   \n",
       "3          295.485596   0.000005  409.990204  ...    0.000109    427  342   \n",
       "4          346.193329   0.000005  409.990204  ...    0.000045    427  342   \n",
       "5          346.206726   0.000006  408.523132  ...    0.000006    427  342   \n",
       "6          346.193329   0.000006  406.798920  ...    0.000006    427  342   \n",
       "7          346.205170   0.000007  406.798920  ...    0.000007    427  342   \n",
       "8          346.200562   0.000483  408.165070  ...    0.000179    427  342   \n",
       "9          346.205170   0.000007  408.165070  ...    0.000005    427  342   \n",
       "\n",
       "scorer                                                                      \n",
       "bodyparts            starting_point                 border                  \n",
       "coords    likelihood              x    y likelihood      x    y likelihood  \n",
       "0                  1            426  307          1      1  313          1  \n",
       "1                  1            426  307          1      1  313          1  \n",
       "2                  1            426  307          1      1  313          1  \n",
       "3                  1            426  307          1      1  313          1  \n",
       "4                  1            426  307          1      1  313          1  \n",
       "5                  1            426  307          1      1  313          1  \n",
       "6                  1            426  307          1      1  313          1  \n",
       "7                  1            426  307          1      1  313          1  \n",
       "8                  1            426  307          1      1  313          1  \n",
       "9                  1            426  307          1      1  313          1  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tkinter import *\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "# run the gui to choose file\n",
    "root = Tk()\n",
    "root.update()\n",
    "file = askopenfilename(filetypes =[('DLC Tracking Files', '*.h5')])\n",
    "root.destroy() \n",
    "\n",
    "# open file in pandas and add new values for the reference points\n",
    "table = pd.read_hdf(file, key = 'df_with_missing')\n",
    "\n",
    "# add manual values for the pellet\n",
    "table[(table.columns.get_level_values(0)[0], 'pellet', 'x')]=427\n",
    "table[(table.columns.get_level_values(0)[0], 'pellet', 'y')]=342\n",
    "table[(table.columns.get_level_values(0)[0], 'pellet', 'likelihood')]=1\n",
    "\n",
    "# add manual values for the starting point\n",
    "table[(table.columns.get_level_values(0)[0], 'starting_point', 'x')]=426\n",
    "table[(table.columns.get_level_values(0)[0], 'starting_point', 'y')]=307\n",
    "table[(table.columns.get_level_values(0)[0], 'starting_point', 'likelihood')]=1\n",
    "\n",
    "# add manual values for the border\n",
    "table[(table.columns.get_level_values(0)[0], 'border', 'x')]=1\n",
    "table[(table.columns.get_level_values(0)[0], 'border', 'y')]=313\n",
    "table[(table.columns.get_level_values(0)[0], 'border', 'likelihood')]=1\n",
    "\n",
    "\n",
    "table.to_hdf(file, key = 'df_with_missing')\n",
    "table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6cb00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tkinter import *\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "# run the gui to choose file\n",
    "root = Tk()\n",
    "root.update()\n",
    "file = askopenfilename(filetypes =[('DLC Tracking Files', '*.h5')])\n",
    "root.destroy() \n",
    "\n",
    "# open file in pandas and add new values for the reference points\n",
    "table = pd.read_hdf(file, key = 'df_with_missing')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d199b5",
   "metadata": {},
   "source": [
    "#### 2. Triangulate the 2d trajectories with Anipose \n",
    "We implemented parts of Anipose Lib and provide the 'simpler' way of performing the triangulation within the Jupiter Notebook. The following two cells allow to perform the Camera Calibration and Camera Triangulation with the help of AniposeLib. However, you still need to manually make videos for camera calibration and point to them in the first step. The output of this step will be the calibration.toml file with camera extrinsics and intrinsics. The path to this file as well as the path to the videos in need for triangulation should be pointed in the second step.\n",
    "\n",
    "This step can also be done outside of the jupyter notebook (as advised in Anipose tutorial), then you need to perform several steps, not including the Anipose calibration: <ol> <li>-  manually make an Anipose directory </li>  <li>-  rename the files after DLC analysis, leaving just the *camX* in the end </li> <li>- transfer the .csv and .h5 files with the tracking results to the Anipose directory </li> <li>- *cd* to this directory and </li> <li>- run from the command prompt *anipose triangulate* </li> </ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc06cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALIBRATING THE CAMERAS\n",
    "\n",
    "import numpy as np\n",
    "from aniposelib.boards import CharucoBoard, Checkerboard\n",
    "from aniposelib.cameras import Camera, CameraGroup\n",
    "from aniposelib.utils import load_pose2d_fnames\n",
    "\n",
    "# modify the following lines to match your project\n",
    "####################################################\n",
    "vidnames = [['Anipose Project/2019-08-02/calibration/2-Batch7_6_calibration-camB.mp4'], #the videos should have the same name\n",
    "            ['Anipose Project/2019-08-02/calibration/2-Batch7_6_calibration-camC.mp4']] # ending with the camX suffix\n",
    "\n",
    "cam_names = ['B', 'C']                                  # use A, B or C after -cam to distinguish between views\n",
    "####################################################\n",
    "\n",
    "n_cams = len(vidnames)\n",
    "\n",
    "board = Checkerboard(4, 4,\n",
    "                     square_length=5,  manually_verify=True) # here, in mm but any unit works, we used 4x4 checkerboard for calibration\n",
    "                     \n",
    "\n",
    "\n",
    "# the videos provided are fisheye, so we need the fisheye option\n",
    "cgroup = CameraGroup.from_names(cam_names, fisheye=False)\n",
    "\n",
    "# this will take about 15 minutes (mostly due to detection)\n",
    "# it will detect the checkerboard board in the videos,\n",
    "# then calibrate the cameras based on the detections, using iterative bundle adjustment\n",
    "# NOTE: this script will open the simple interface to check the accurate detection of checkerboards\n",
    "# use A or X to accept/reject the boards\n",
    "cgroup.calibrate_videos(vidnames, board)\n",
    "\n",
    "# if you need to save and load\n",
    "# example saving and loading for later\n",
    "cgroup.dump('calibration.toml')\n",
    "\n",
    "## example of loading calibration from a file\n",
    "## you can also load the provided file if you don't want to wait 15 minutes\n",
    "#cgroup = CameraGroup.load('calibration.toml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRIANGULATING THE ACTUAL COORDINATES WITH THE USE OF CALIBRATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from aniposelib.boards import CharucoBoard, Checkerboard\n",
    "from aniposelib.cameras import Camera, CameraGroup\n",
    "from aniposelib.utils import load_pose2d_fnames\n",
    "from coord_correct import correct_coordinate_frame, get_median, ortho, proj\n",
    "\n",
    "#modify the following lines to match your project\n",
    "##################################################\n",
    "output_fname = '//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/10wo timepoint video analysis/Comparing networks/RightHanded/ncb1284-R-reaching7.csv'                     #the file to save the result with 3d trajectory\n",
    "cgroup = CameraGroup.load('//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/10wo timepoint video analysis/Comparing networks/RightHanded/calibration-right.toml')              #the file with calibration from previous step\n",
    "score_threshold = 0.1                                               #lower threshold to accept the point tracked\n",
    "\n",
    "# loading data from DLC\n",
    "fname_dict = {                                                      #name to files you want to triangulate\n",
    "    'A': '//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/10wo timepoint video analysis/Comparing networks/RightHanded/DLC_resnet50_Left_MirrorJun28shuffle1_1000000_filtered_ncb1284-R-camA.h5',\n",
    "    'B': '//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/10wo timepoint video analysis/Comparing networks/RightHanded/DLC_resnet50_Center_MirrorJun30shuffle1_990000_filtered_ncb1284-R-camB.h5'\n",
    "}\n",
    "###################################################\n",
    "\n",
    "d = load_pose2d_fnames(fname_dict, cam_names=cgroup.get_names())\n",
    "\n",
    "n_cams, n_points, n_joints, _ = d['points'].shape\n",
    "points = d['points']\n",
    "scores = d['scores']\n",
    "bodyparts = d['bodyparts']\n",
    "\n",
    "# remove points that are below threshold\n",
    "points[scores < score_threshold] = np.nan\n",
    "\n",
    "points_flat = points.reshape(n_cams, -1, 2)\n",
    "scores_flat = scores.reshape(n_cams, -1)\n",
    "\n",
    "p3ds_flat = cgroup.triangulate(points_flat, progress=True)\n",
    "reprojerr_flat = cgroup.reprojection_error(p3ds_flat, points_flat, mean=True)\n",
    "\n",
    "p3ds = p3ds_flat.reshape(n_points, n_joints, 3)\n",
    "reprojerr = reprojerr_flat.reshape(n_points, n_joints)\n",
    "\n",
    "\n",
    "######################################################\n",
    "# This part of the code (if triangulation is set to True) is correcting the coordinate frame to match the axes entered and takes\n",
    "# the reference point as zero, substracting the median of the corresponding column from all points tracked\n",
    "\n",
    "triangulation = True\n",
    "axes = [\n",
    "    [\"x\", \"starting_point\", \"pellet\"],\n",
    "    [\"y\", \"starting_point\", \"border\"]\n",
    "]\n",
    "\n",
    "reference_point = \"pellet\"\n",
    "\n",
    "config=pd.DataFrame(columns=['triangulation'], index=['axes','reference_point'])\n",
    "config['triangulation']['axes'] = [\n",
    "    [\"x\", \"starting_point\", \"pellet\"],\n",
    "    [\"y\", \"starting_point\", \"border\"]\n",
    "]\n",
    "\n",
    "config['triangulation']['reference_point'] = \"pellet\"\n",
    "\n",
    "###\n",
    "\n",
    "if triangulation is True:\n",
    "    all_points_3d_adj, M, center = correct_coordinate_frame(config, p3ds, bodyparts)\n",
    "else:\n",
    "    all_points_3d_adj = p3ds\n",
    "    M = np.identity(3)\n",
    "    center = np.zeros(3)\n",
    "######################################################\n",
    "\n",
    "\n",
    "# concatenating everything in the dataframe and saving to csv   \n",
    "good_points = ~np.isnan(points[:, :, :, 0])\n",
    "num_cams = np.sum(good_points, axis=0).astype('float')\n",
    "scores[~good_points] = 2\n",
    "scores_3d = np.min(scores, axis=0)\n",
    "\n",
    "\n",
    "dout = pd.DataFrame()\n",
    "for bp_num, bp in enumerate(bodyparts):\n",
    "    for ax_num, axis in enumerate(['x','y','z']):\n",
    "        dout[bp + '_' + axis] = all_points_3d_adj[:, bp_num, ax_num]\n",
    "    dout[bp + '_error'] = reprojerr[:, bp_num]\n",
    "    dout[bp + '_ncams'] = num_cams[:, bp_num]\n",
    "    dout[bp + '_score'] = scores_3d[:, bp_num]\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        dout['M_{}{}'.format(i, j)] = M[i, j]\n",
    "\n",
    "for i in range(3):\n",
    "    dout['center_{}'.format(i)] = center[i]\n",
    "\n",
    "dout['fnum'] = np.arange(n_points)\n",
    "\n",
    "dout.to_csv(output_fname, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c6db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASIC VISUAL VERIFICATION OF THE 3D COORDINATES\n",
    "%matplotlib notebook\n",
    "# for more dynamic programs like this one we are switching to the interactive matplotlib backend\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.figure(figsize=(9.4, 6))\n",
    "axes1 = fig.add_subplot(111)\n",
    "axes2 = axes1.twinx()\n",
    "#axes1.plot(p3ds[:, 0, 0])        # you can generate as many plots from p3ds dataframe as you want, for each part triangulated\n",
    "axes1.plot(p3ds[:, 0, 1])\n",
    "#axes2.plot(p3ds[:, 0, 2])\n",
    "plt.xlabel(\"Time (frames)\")\n",
    "plt.ylabel(\"Coordinate (mm)\")\n",
    "plt.title(\"x, y, z coordinates of {}\".format(bodyparts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada222d5",
   "metadata": {},
   "source": [
    "### The end of the first notebook. Proceed to part 2\n",
    "The result of this notebook is the p3ds dataframe with all the coordinates triangulated.\n",
    "It is saved as a csv file in the designated location, so you can open it in the next notebook for\n",
    "automatic/manual analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
