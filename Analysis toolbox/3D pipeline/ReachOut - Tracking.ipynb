{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d363a9d3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# ReachOut - reaching movement kinematics analysis - Part1 (Tracking)\n",
    "___\n",
    "\n",
    "## This Notebook contains the scripts to prepare the video for analysis with DEEPLABCUT and Anipose and generate the 3d trajectories:\n",
    "\n",
    "### 0. Cut the video from the FLIR camera into three parts - three different views of the reach\n",
    "We are recording the videos of the reaching task using the single high speed widefield FLIR camera and multiple mirrors. So the first part is to cut single video into parts corresponding to the Rightside view (CamA), Center view (CamB), Leftside view (CamC) with respect to the animal paw. This eliminates the need for synchronisation of different videos, but adds this additional step before we can analyze our videos with DLC. \n",
    "As a result of this step you should get three video files with the names -camA, -camB, -camC.\n",
    "\n",
    "### 1. Analyze the video with DeepLabCut\n",
    "This step is performed using open-source DEEPLABCUT and Anipose packages, the tutorial and more info on them can be found on GitHub\n",
    "__[https://github.com/DeepLabCut](DEEPLABCUT)__\n",
    "In short, the first command should launch DeepLabCut interface which guides you through creating the network, training dataset from your videos (selecting subset of frames and manually labeling the points of interest), training & evaluating the network and finally applying it to all your videos. The final output should be a *.csv* file for each video with coordinates (in pixel dimensions) for all the points tracked by the network. We added some code to open, check this table and insert necessary columns with static points for triangulation.   \n",
    "\n",
    "It should be further noted that despite the Anipose documentation advice to use single DLC network for all views we found that better results are achieved with two separate networks trained on the side(1) and front(2) view. That means we start DLC analysis for our videos twice - first time for all the CamA/C videos, and the second time for CamB.\n",
    "As a result we get two separate sets of _*.csv *.h5_ files. For the next step they should have the same name with the difference of last prefix \"A,B,C\" and copied together to one folder accessible by this Notebook. \n",
    "\n",
    "### 2. Triangulate the 2d trajectories with Anipose and check the quality of the 3d trajectory \n",
    "This step is done using the open-source Anipose package, so the tutorial and more info on them can be found on GitHub\n",
    "__[https://github.com/lambdaloop/anipose](ANIPOSE)__\n",
    "Here we provide the code snippet (from Anipose lib) to calibrate the camera intrinsics using prerecorded videos or checkerboard pattern and use the resulting *calibration.toml* file to triangulate coordinates for the paw in absolute space.\n",
    "\n",
    "If you want to use Anipose interface for triangulation you should manually put the resulting .csv and .h5 files into anipose project folder _*/Anipose project/pose-2d before_ before starting the triangulation command _anipose triangulate_\n",
    "\n",
    "As a result of this step you should get the single _*.csv_ file for each session, containing absolute x,y,z coordinates, that we will use in the next notebook\n",
    "\n",
    "\n",
    "  \n",
    "  ***\n",
    "  ___\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5c4e4",
   "metadata": {},
   "source": [
    "# Tracking and triagulation part\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1160162",
   "metadata": {},
   "source": [
    "#### 0. Cut the video from the FLIR camera \n",
    "Note that we have two scripts for two use cases. If you need to process just a single file, type _from video_split import video_split_ and if you want to process all files in a certain folder type _from video_split_folder import video_split_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5752cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to do bulk analysis on all the files in a folder - uncomment the second line and comment the first one \n",
    "#from video_split import video_split           #for single file uncomment this line\n",
    "from video_split_folder import video_split     #for multiple files \n",
    "\n",
    "video_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcace58",
   "metadata": {},
   "source": [
    "#### 1.  Analyze the video with DeepLabCut\n",
    "The following line should open DLC interface. Open the project containing your trained network, open the analyze video layout and select the videos you want to track (supposedly, either all camA, camB, or camC videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3dfdfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "C:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tf_slim\\layers\\layers.py:684: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  outputs = layer.apply(inputs, training=is_training)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1279it [00:42, 30.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving plots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████████▊                                                                                    | 244/1279 [02:20<09:56,  1.73it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to gain raw access to bitmap data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\gui\\evaluate_network.py:266\u001b[0m, in \u001b[0;36mEvaluate_network.evaluate_network\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_scoremaps\u001b[38;5;241m.\u001b[39mGetStringSelection() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shuffle \u001b[38;5;129;01min\u001b[39;00m Shuffles:\n\u001b[1;32m--> 266\u001b[0m         \u001b[43mdeeplabcut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_save_all_maps\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbodyparts) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbodyparts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\visualizemaps.py:499\u001b[0m, in \u001b[0;36mextract_save_all_maps\u001b[1;34m(config, shuffle, trainingsetindex, comparisonbodyparts, extract_paf, all_paf_in_one, gputouse, rescale, Indices, modelprefix, dest_folder)\u001b[0m\n\u001b[0;32m    489\u001b[0m temp \u001b[38;5;241m=\u001b[39m dest_path\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    490\u001b[0m     imname\u001b[38;5;241m=\u001b[39mimname,\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28mmap\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscmap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    495\u001b[0m     snap\u001b[38;5;241m=\u001b[39msnap,\n\u001b[0;32m    496\u001b[0m )\n\u001b[0;32m    497\u001b[0m fig1\u001b[38;5;241m.\u001b[39msavefig(temp)\n\u001b[1;32m--> 499\u001b[0m fig2, _ \u001b[38;5;241m=\u001b[39m \u001b[43mvisualize_locrefs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocref_x_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocref_y_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m temp \u001b[38;5;241m=\u001b[39m dest_path\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    501\u001b[0m     imname\u001b[38;5;241m=\u001b[39mimname,\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28mmap\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocref\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    506\u001b[0m     snap\u001b[38;5;241m=\u001b[39msnap,\n\u001b[0;32m    507\u001b[0m )\n\u001b[0;32m    508\u001b[0m fig2\u001b[38;5;241m.\u001b[39msavefig(temp)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\visualizemaps.py:305\u001b[0m, in \u001b[0;36mvisualize_locrefs\u001b[1;34m(image, scmap, locref_x, locref_y, step, zoom_width)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_locrefs\u001b[39m(image, scmap, locref_x, locref_y, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, zoom_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m     fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mvisualize_scoremaps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscmap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(np\u001b[38;5;241m.\u001b[39marange(locref_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]), np\u001b[38;5;241m.\u001b[39marange(locref_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m    307\u001b[0m     M \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(locref_x\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\visualizemaps.py:298\u001b[0m, in \u001b[0;36mvisualize_scoremaps\u001b[1;34m(image, scmap)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_scoremaps\u001b[39m(image, scmap):\n\u001b[0;32m    297\u001b[0m     ny, nx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mshape(image)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m--> 298\u001b[0m     fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mform_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mny\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     ax\u001b[38;5;241m.\u001b[39mimshow(image)\n\u001b[0;32m    300\u001b[0m     ax\u001b[38;5;241m.\u001b[39mimshow(scmap, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\visualizemaps.py:287\u001b[0m, in \u001b[0;36mform_figure\u001b[1;34m(nx, ny)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mform_figure\u001b[39m(nx, ny):\n\u001b[1;32m--> 287\u001b[0m     fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubplots\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframeon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m     ax\u001b[38;5;241m.\u001b[39mset_xlim(\u001b[38;5;241m0\u001b[39m, nx)\n\u001b[0;32m    289\u001b[0m     ax\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m0\u001b[39m, ny)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\pyplot.py:1434\u001b[0m, in \u001b[0;36msubplots\u001b[1;34m(nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubplots\u001b[39m(nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m, sharex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sharey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1301\u001b[0m              subplot_kw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, gridspec_kw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfig_kw):\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1303\u001b[0m \u001b[38;5;124;03m    Create a figure and a set of subplots.\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1432\u001b[0m \n\u001b[0;32m   1433\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1434\u001b[0m     fig \u001b[38;5;241m=\u001b[39m \u001b[43mfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfig_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1435\u001b[0m     axs \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39mnrows, ncols\u001b[38;5;241m=\u001b[39mncols, sharex\u001b[38;5;241m=\u001b[39msharex, sharey\u001b[38;5;241m=\u001b[39msharey,\n\u001b[0;32m   1436\u001b[0m                        squeeze\u001b[38;5;241m=\u001b[39msqueeze, subplot_kw\u001b[38;5;241m=\u001b[39msubplot_kw,\n\u001b[0;32m   1437\u001b[0m                        gridspec_kw\u001b[38;5;241m=\u001b[39mgridspec_kw)\n\u001b[0;32m   1438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fig, axs\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\pyplot.py:787\u001b[0m, in \u001b[0;36mfigure\u001b[1;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(allnums) \u001b[38;5;241m==\u001b[39m max_open_warning \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    779\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_external(\n\u001b[0;32m    780\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_open_warning\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m figures have been opened. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFigures created through the pyplot interface \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarning, see the rcParam `figure.max_open_warning`).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m)\n\u001b[1;32m--> 787\u001b[0m manager \u001b[38;5;241m=\u001b[39m \u001b[43mnew_figure_manager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfigsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframeon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframeon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFigureClass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFigureClass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    791\u001b[0m fig \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fig_label:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\pyplot.py:306\u001b[0m, in \u001b[0;36mnew_figure_manager\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m\"\"\"Create a new figure manager instance.\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_backend_mod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_figure_manager\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\backends\\backend_wx.py:1406\u001b[0m, in \u001b[0;36m_BackendWx.new_figure_manager\u001b[1;34m(cls, num, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;66;03m# Retain a reference to the app object so that it does not get\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;66;03m# garbage collected.\u001b[39;00m\n\u001b[0;32m   1405\u001b[0m     _BackendWx\u001b[38;5;241m.\u001b[39m_theWxApp \u001b[38;5;241m=\u001b[39m wxapp\n\u001b[1;32m-> 1406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_figure_manager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\backend_bases.py:3494\u001b[0m, in \u001b[0;36m_Backend.new_figure_manager\u001b[1;34m(cls, num, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3492\u001b[0m fig_cls \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFigureClass\u001b[39m\u001b[38;5;124m'\u001b[39m, Figure)\n\u001b[0;32m   3493\u001b[0m fig \u001b[38;5;241m=\u001b[39m fig_cls(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_figure_manager_given_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\backends\\backend_wx.py:1410\u001b[0m, in \u001b[0;36m_BackendWx.new_figure_manager_given_figure\u001b[1;34m(cls, num, figure)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_figure_manager_given_figure\u001b[39m(\u001b[38;5;28mcls\u001b[39m, num, figure):\n\u001b[1;32m-> 1410\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_frame_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1411\u001b[0m     figmgr \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mget_figure_manager()\n\u001b[0;32m   1412\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mis_interactive():\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\backends\\backend_wx.py:918\u001b[0m, in \u001b[0;36mFigureFrameWx.__init__\u001b[1;34m(self, num, fig)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;66;03m# By adding toolbar in sizer, we are able to put it at the bottom\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;66;03m# of the frame - so appearance is closer to GTK version\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigmgr \u001b[38;5;241m=\u001b[39m FigureManagerWx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanvas, num, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 918\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_toolbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigmgr\u001b[38;5;241m.\u001b[39mtoolmanager:\n\u001b[0;32m    921\u001b[0m     backend_tools\u001b[38;5;241m.\u001b[39madd_tools_to_manager(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigmgr\u001b[38;5;241m.\u001b[39mtoolmanager)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\backends\\backend_wx.py:948\u001b[0m, in \u001b[0;36mFigureFrameWx._get_toolbar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_toolbar\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoolbar\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoolbar2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 948\u001b[0m         toolbar \u001b[38;5;241m=\u001b[39m \u001b[43mNavigationToolbar2Wx\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoolbar\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoolmanager\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    950\u001b[0m         toolbar \u001b[38;5;241m=\u001b[39m ToolbarWx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolmanager, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\backends\\backend_wx.py:1094\u001b[0m, in \u001b[0;36mNavigationToolbar2Wx.__init__\u001b[1;34m(self, canvas, coordinates)\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAddSeparator()\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwx_ids[text] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1092\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAddTool(\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m-> 1094\u001b[0m             bitmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_icon\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimage_file\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1095\u001b[0m             bmpDisabled\u001b[38;5;241m=\u001b[39mwx\u001b[38;5;241m.\u001b[39mNullBitmap,\n\u001b[0;32m   1096\u001b[0m             label\u001b[38;5;241m=\u001b[39mtext, shortHelp\u001b[38;5;241m=\u001b[39mtooltip_text,\n\u001b[0;32m   1097\u001b[0m             kind\u001b[38;5;241m=\u001b[39m(wx\u001b[38;5;241m.\u001b[39mITEM_CHECK \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPan\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZoom\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1098\u001b[0m                   \u001b[38;5;28;01melse\u001b[39;00m wx\u001b[38;5;241m.\u001b[39mITEM_NORMAL))\n\u001b[0;32m   1099\u001b[0m         \u001b[38;5;241m.\u001b[39mId)\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBind(wx\u001b[38;5;241m.\u001b[39mEVT_TOOL, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, callback),\n\u001b[0;32m   1101\u001b[0m               \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwx_ids[text])\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinates \u001b[38;5;241m=\u001b[39m coordinates\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\matplotlib\\backends\\backend_wx.py:1135\u001b[0m, in \u001b[0;36mNavigationToolbar2Wx._icon\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     black_mask \u001b[38;5;241m=\u001b[39m (image[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mall(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1134\u001b[0m     image[black_mask, :\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m (fg\u001b[38;5;241m.\u001b[39mRed(), fg\u001b[38;5;241m.\u001b[39mGreen(), fg\u001b[38;5;241m.\u001b[39mBlue())\n\u001b[1;32m-> 1135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBitmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFromBufferRGBA\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to gain raw access to bitmap data."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running  DLC_resnet50_LeftCenter_mirror_miniscopeNov27shuffle1_930000  with # of training iterations: 930000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "C:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tf_slim\\layers\\layers.py:684: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  outputs = layer.apply(inputs, training=is_training)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1279it [00:35, 35.57it/s]\n"
     ]
    },
    {
     "ename": "HDF5ExtError",
     "evalue": "HDF5 error back trace\n\n  File \"D:\\pytables_hdf5\\CMake-hdf5-1.10.5\\hdf5-1.10.5\\src\\H5F.c\", line 444, in H5Fcreate\n    unable to create file\n  File \"D:\\pytables_hdf5\\CMake-hdf5-1.10.5\\hdf5-1.10.5\\src\\H5Fint.c\", line 1509, in H5F_open\n    unable to open file: time = Fri Mar 22 14:18:43 2024\n, name = 'C:\\Users\\daniil.berezhnoi.VAI\\Videos\\3d plot - manual assembly\\LeftCenter_mirror_miniscope-Daniil-2023-11-27\\evaluation-results\\iteration-1\\LeftCenter_mirror_miniscopeNov27-trainset95shuffle1\\DLC_resnet50_LeftCenter_mirror_miniscopeNov27shuffle1_930000-snapshot-930000.h5', tent_flags = 13\n  File \"D:\\pytables_hdf5\\CMake-hdf5-1.10.5\\hdf5-1.10.5\\src\\H5FD.c\", line 734, in H5FD_open\n    open failed\n  File \"D:\\pytables_hdf5\\CMake-hdf5-1.10.5\\hdf5-1.10.5\\src\\H5FDsec2.c\", line 346, in H5FD_sec2_open\n    unable to open file: name = 'C:\\Users\\daniil.berezhnoi.VAI\\Videos\\3d plot - manual assembly\\LeftCenter_mirror_miniscope-Daniil-2023-11-27\\evaluation-results\\iteration-1\\LeftCenter_mirror_miniscopeNov27-trainset95shuffle1\\DLC_resnet50_LeftCenter_mirror_miniscopeNov27shuffle1_930000-snapshot-930000.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302\n\nEnd of HDF5 error back trace\n\nUnable to open/create file 'C:\\Users\\daniil.berezhnoi.VAI\\Videos\\3d plot - manual assembly\\LeftCenter_mirror_miniscope-Daniil-2023-11-27\\evaluation-results\\iteration-1\\LeftCenter_mirror_miniscopeNov27-trainset95shuffle1\\DLC_resnet50_LeftCenter_mirror_miniscopeNov27shuffle1_930000-snapshot-930000.h5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHDF5ExtError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\gui\\evaluate_network.py:270\u001b[0m, in \u001b[0;36mEvaluate_network.evaluate_network\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbodyparts) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbodyparts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 270\u001b[0m \u001b[43mdeeplabcut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mShuffles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mShuffles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#trainingsetindex=trainingsetindex,\u001b[39;49;00m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplotting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplotting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomparisonbodyparts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbodyparts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\core\\evaluate.py:832\u001b[0m, in \u001b[0;36mevaluate_network\u001b[1;34m(config, Shuffles, trainingsetindex, plotting, show_errors, comparisonbodyparts, gputouse, rescale, modelprefix)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;66;03m# Saving results\u001b[39;00m\n\u001b[0;32m    829\u001b[0m DataMachine \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m    830\u001b[0m     PredicteData, columns\u001b[38;5;241m=\u001b[39mindex, index\u001b[38;5;241m=\u001b[39mData\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m    831\u001b[0m )\n\u001b[1;32m--> 832\u001b[0m \u001b[43mDataMachine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresultsfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdf_with_missing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalysis is done and the results are stored (see evaluation-results) for snapshot: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    836\u001b[0m     Snapshots[snapindex],\n\u001b[0;32m    837\u001b[0m )\n\u001b[0;32m    838\u001b[0m DataCombined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[0;32m    839\u001b[0m     [Data\u001b[38;5;241m.\u001b[39mT, DataMachine\u001b[38;5;241m.\u001b[39mT], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    840\u001b[0m )\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\pandas\\core\\generic.py:2763\u001b[0m, in \u001b[0;36mNDFrame.to_hdf\u001b[1;34m(self, path_or_buf, key, mode, complevel, complib, append, format, index, min_itemsize, nan_rep, dropna, data_columns, errors, encoding)\u001b[0m\n\u001b[0;32m   2759\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pytables\n\u001b[0;32m   2761\u001b[0m \u001b[38;5;66;03m# Argument 3 to \"to_hdf\" has incompatible type \"NDFrame\"; expected\u001b[39;00m\n\u001b[0;32m   2762\u001b[0m \u001b[38;5;66;03m# \"Union[DataFrame, Series]\" [arg-type]\u001b[39;00m\n\u001b[1;32m-> 2763\u001b[0m \u001b[43mpytables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_hdf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2766\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   2767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomplevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomplib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2771\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_itemsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_itemsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2777\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2779\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\pandas\\io\\pytables.py:311\u001b[0m, in \u001b[0;36mto_hdf\u001b[1;34m(path_or_buf, key, value, mode, complevel, complib, append, format, index, min_itemsize, nan_rep, dropna, data_columns, errors, encoding)\u001b[0m\n\u001b[0;32m    309\u001b[0m path_or_buf \u001b[38;5;241m=\u001b[39m stringify_path(path_or_buf)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mHDFStore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplib\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m store:\n\u001b[0;32m    314\u001b[0m         f(store)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\pandas\\io\\pytables.py:591\u001b[0m, in \u001b[0;36mHDFStore.__init__\u001b[1;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fletcher32 \u001b[38;5;241m=\u001b[39m fletcher32\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 591\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\pandas\\io\\pytables.py:740\u001b[0m, in \u001b[0;36mHDFStore.open\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    734\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot open HDF5 file, which is already opened, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    736\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meven in read-only mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m     )\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m--> 740\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43mtables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tables\\file.py:315\u001b[0m, in \u001b[0;36mopen_file\u001b[1;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    311\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is already opened.  Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    312\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose it before reopening in write mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# Finally, create the File instance, and return it\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_uep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tables\\file.py:778\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m params\n\u001b[0;32m    777\u001b[0m \u001b[38;5;66;03m# Now, it is time to initialize the File extension\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_g_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;66;03m# Check filters and set PyTables format version for new files.\u001b[39;00m\n\u001b[0;32m    781\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v_new\n",
      "File \u001b[1;32mtables\\hdf5extension.pyx:492\u001b[0m, in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mHDF5ExtError\u001b[0m: HDF5 error back trace\n\n  File \"D:\\pytables_hdf5\\CMake-hdf5-1.10.5\\hdf5-1.10.5\\src\\H5F.c\", line 444, in H5Fcreate\n    unable to create file\n  File \"D:\\pytables_hdf5\\CMake-hdf5-1.10.5\\hdf5-1.10.5\\src\\H5Fint.c\", line 1509, in H5F_open\n    unable to open file: time = Fri Mar 22 14:18:43 2024\n, name = 'C:\\Users\\daniil.berezhnoi.VAI\\Videos\\3d plot - manual assembly\\LeftCenter_mirror_miniscope-Daniil-2023-11-27\\evaluation-results\\iteration-1\\LeftCenter_mirror_miniscopeNov27-trainset95shuffle1\\DLC_resnet50_LeftCenter_mirror_miniscopeNov27shuffle1_930000-snapshot-930000.h5', tent_flags = 13\n  File \"D:\\pytables_hdf5\\CMake-hdf5-1.10.5\\hdf5-1.10.5\\src\\H5FD.c\", line 734, in H5FD_open\n    open failed\n  File \"D:\\pytables_hdf5\\CMake-hdf5-1.10.5\\hdf5-1.10.5\\src\\H5FDsec2.c\", line 346, in H5FD_sec2_open\n    unable to open file: name = 'C:\\Users\\daniil.berezhnoi.VAI\\Videos\\3d plot - manual assembly\\LeftCenter_mirror_miniscope-Daniil-2023-11-27\\evaluation-results\\iteration-1\\LeftCenter_mirror_miniscopeNov27-trainset95shuffle1\\DLC_resnet50_LeftCenter_mirror_miniscopeNov27shuffle1_930000-snapshot-930000.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302\n\nEnd of HDF5 error back trace\n\nUnable to open/create file 'C:\\Users\\daniil.berezhnoi.VAI\\Videos\\3d plot - manual assembly\\LeftCenter_mirror_miniscope-Daniil-2023-11-27\\evaluation-results\\iteration-1\\LeftCenter_mirror_miniscopeNov27-trainset95shuffle1\\DLC_resnet50_LeftCenter_mirror_miniscopeNov27shuffle1_930000-snapshot-930000.h5'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-930000 for model C:\\Users\\daniil.berezhnoi.VAI\\Videos\\3d plot - manual assembly\\LeftCenter_mirror_miniscope-Daniil-2023-11-27\\dlc-models\\iteration-1\\LeftCenter_mirror_miniscopeNov27-trainset95shuffle1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "C:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\tf_slim\\layers\\layers.py:684: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  outputs = layer.apply(inputs, training=is_training)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_11_26_23_ncb1796.mp4_fixed-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_11_26_23_ncb1796.mp4_fixed-camB.mp4\n",
      "Duration of video [s]:  1394.64 , recorded with  25.0 fps!\n",
      "Overall # of frames:  34866  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35148it [08:12, 71.41it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_13_15_25_ncb1788-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_13_15_25_ncb1788-camB.mp4\n",
      "Duration of video [s]:  1561.68 , recorded with  25.0 fps!\n",
      "Overall # of frames:  39042  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39390it [09:10, 71.53it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_14_15_52_ncb1789-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_14_15_52_ncb1789-camB.mp4\n",
      "Duration of video [s]:  1800.56 , recorded with  25.0 fps!\n",
      "Overall # of frames:  45014  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45450it [10:37, 71.25it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_14_57_00_ncb1899.mp4_fixed-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_14_57_00_ncb1899.mp4_fixed-camB.mp4\n",
      "Duration of video [s]:  478.64 , recorded with  25.0 fps!\n",
      "Overall # of frames:  11966  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12019it [02:51, 69.99it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_15_57_47_ncb1898-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_15_57_47_ncb1898-camB.mp4\n",
      "Duration of video [s]:  1779.2 , recorded with  25.0 fps!\n",
      "Overall # of frames:  44480  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44844it [10:31, 71.04it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_16_08_27_ncb1841-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_16_08_27_ncb1841-camB.mp4\n",
      "Duration of video [s]:  1003.92 , recorded with  25.0 fps!\n",
      "Overall # of frames:  25098  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25250it [05:57, 70.53it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_16_43_09_ncb1847.mp4_fixed-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_16_43_09_ncb1847.mp4_fixed-camB.mp4\n",
      "Duration of video [s]:  1574.8 , recorded with  25.0 fps!\n",
      "Overall # of frames:  39370  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39693it [09:21, 70.66it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_16_56_29_ncb1818.mp4_fixed-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_16_56_29_ncb1818.mp4_fixed-camB.mp4\n",
      "Duration of video [s]:  2099.76 , recorded with  25.0 fps!\n",
      "Overall # of frames:  52494  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52924it [12:25, 70.96it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_17_15_10_ncb1855.mp4_fixed-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_17_15_10_ncb1855.mp4_fixed-camB.mp4\n",
      "Duration of video [s]:  985.76 , recorded with  25.0 fps!\n",
      "Overall # of frames:  24644  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24846it [05:50, 70.80it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_17_26_35_ncb1895-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_17_26_35_ncb1895-camB.mp4\n",
      "Duration of video [s]:  1769.0 , recorded with  25.0 fps!\n",
      "Overall # of frames:  44225  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44642it [10:40, 69.72it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo...\n",
      "Saving csv poses!\n",
      "Starting to analyze %  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_17_59_08_ncb1909-camB.mp4\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_17_59_08_ncb1909-camB.mp4\n",
      "Duration of video [s]:  849.52 , recorded with  25.0 fps!\n",
      "Overall # of frames:  21238  found with (before cropping) frame dimensions:  824 408\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21412it [05:09, 69.14it/s]                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo...\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_11_26_23_ncb1796.mp4_fixed-camB.mp4\n",
      "Saving filtered csv poses!\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_13_15_25_ncb1788-camB.mp4\n",
      "Saving filtered csv poses!\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_14_15_52_ncb1789-camB.mp4\n",
      "Saving filtered csv poses!\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_14_57_00_ncb1899.mp4_fixed-camB.mp4\n",
      "Saving filtered csv poses!\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_15_57_47_ncb1898-camB.mp4\n",
      "Saving filtered csv poses!\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_16_08_27_ncb1841-camB.mp4\n",
      "Saving filtered csv poses!\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_16_43_09_ncb1847.mp4_fixed-camB.mp4\n",
      "Saving filtered csv poses!\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_16_56_29_ncb1818.mp4_fixed-camB.mp4\n",
      "Saving filtered csv poses!\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_17_15_10_ncb1855.mp4_fixed-camB.mp4\n",
      "Saving filtered csv poses!\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_17_26_35_ncb1895-camB.mp4\n",
      "Saving filtered csv poses!\n",
      "Filtering with median model \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_17_59_08_ncb1909-camB.mp4\n",
      "Saving filtered csv poses!\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_11_26_23_ncb1796.mp4_fixed-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_13_15_25_ncb1788-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_14_15_52_ncb1789-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_14_57_00_ncb1899.mp4_fixed-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_15_57_47_ncb1898-camB.mp4 and data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\utils\\plotting.py:73: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig4 = plt.figure()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_16_08_27_ncb1841-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_16_43_09_ncb1847.mp4_fixed-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_16_56_29_ncb1818.mp4_fixed-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_17_15_10_ncb1855.mp4_fixed-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_17_26_35_ncb1895-camB.mp4 and data.\n",
      "Loading  \\\\pn.vai.org\\projects_primary\\chu\\Daniil Berezhnoi\\Behaviour\\ReachingTask\\Reaching+IT neurons Ca\\12wo\\mj_17_59_08_ncb1909-camB.mp4 and data.\n",
      "Plots created! Please check the directory \"plot-poses\" within the video directory\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "deeplabcut.launch_dlc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c807580",
   "metadata": {},
   "source": [
    "The following line can be used to open, look through and correct the DLC output files with tracking using the standard pandas functions (optional). We're using this function to add reference points for **triangulation** step: pellet, starting_point and border of the training box. Coordinates for this points in the video are entered manually and used during **triangulation** to construct primary axes: forward (X), sideward (Y) and upward (Z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d9c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tkinter import *\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "# run the gui to choose file\n",
    "root = Tk()\n",
    "root.update()\n",
    "file = askopenfilename(filetypes =[('DLC Tracking Files', '*.h5')])\n",
    "root.destroy() \n",
    "\n",
    "# open file in pandas and add new values for the reference points\n",
    "table = pd.read_hdf(file, key = 'df_with_missing')\n",
    "\n",
    "# add manual values for the pellet\n",
    "table[(table.columns.get_level_values(0)[0], 'pellet', 'x')]=427\n",
    "table[(table.columns.get_level_values(0)[0], 'pellet', 'y')]=342\n",
    "table[(table.columns.get_level_values(0)[0], 'pellet', 'likelihood')]=1\n",
    "\n",
    "# add manual values for the starting point\n",
    "table[(table.columns.get_level_values(0)[0], 'starting_point', 'x')]=426\n",
    "table[(table.columns.get_level_values(0)[0], 'starting_point', 'y')]=307\n",
    "table[(table.columns.get_level_values(0)[0], 'starting_point', 'likelihood')]=1\n",
    "\n",
    "# add manual values for the border\n",
    "table[(table.columns.get_level_values(0)[0], 'border', 'x')]=1\n",
    "table[(table.columns.get_level_values(0)[0], 'border', 'y')]=313\n",
    "table[(table.columns.get_level_values(0)[0], 'border', 'likelihood')]=1\n",
    "\n",
    "\n",
    "table.to_hdf(file, key = 'df_with_missing')\n",
    "table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6cb00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tkinter import *\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "# run the gui to choose file\n",
    "root = Tk()\n",
    "root.update()\n",
    "file = askopenfilename(filetypes =[('DLC Tracking Files', '*.h5')])\n",
    "root.destroy() \n",
    "\n",
    "# open file in pandas and add new values for the reference points\n",
    "table = pd.read_hdf(file, key = 'df_with_missing')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d199b5",
   "metadata": {},
   "source": [
    "#### 2. Triangulate the 2d trajectories with Anipose \n",
    "We implemented parts of Anipose Lib and provide the 'simpler' way of performing the triangulation within the Jupiter Notebook. The following two cells allow to perform the Camera Calibration and Camera Triangulation with the help of AniposeLib. However, you still need to manually make videos for camera calibration and point to them in the first step. The output of this step will be the calibration.toml file with camera extrinsics and intrinsics. The path to this file as well as the path to the videos in need for triangulation should be pointed in the second step.\n",
    "\n",
    "This step can also be done outside of the jupyter notebook (as advised in Anipose tutorial), then you need to perform several steps, not including the Anipose calibration: <ol> <li>-  manually make an Anipose directory </li>  <li>-  rename the files after DLC analysis, leaving just the *camX* in the end </li> <li>- transfer the .csv and .h5 files with the tracking results to the Anipose directory </li> <li>- *cd* to this directory and </li> <li>- run from the command prompt *anipose triangulate* </li> </ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc06cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALIBRATING THE CAMERAS\n",
    "\n",
    "import numpy as np\n",
    "from aniposelib.boards import CharucoBoard, Checkerboard\n",
    "from aniposelib.cameras import Camera, CameraGroup\n",
    "from aniposelib.utils import load_pose2d_fnames\n",
    "\n",
    "# modify the following lines to match your project\n",
    "####################################################\n",
    "vidnames = [['Anipose Project/2019-08-02/calibration/2-Batch7_6_calibration-camB.mp4'], #the videos should have the same name\n",
    "            ['Anipose Project/2019-08-02/calibration/2-Batch7_6_calibration-camC.mp4']] # ending with the camX suffix\n",
    "\n",
    "cam_names = ['B', 'C']                                  # use A, B or C after -cam to distinguish between views\n",
    "####################################################\n",
    "\n",
    "n_cams = len(vidnames)\n",
    "\n",
    "board = Checkerboard(4, 4,\n",
    "                     square_length=5,  manually_verify=True) # here, in mm but any unit works, we used 4x4 checkerboard for calibration\n",
    "                     \n",
    "\n",
    "\n",
    "# the videos provided are fisheye, so we need the fisheye option\n",
    "cgroup = CameraGroup.from_names(cam_names, fisheye=False)\n",
    "\n",
    "# this will take about 15 minutes (mostly due to detection)\n",
    "# it will detect the checkerboard board in the videos,\n",
    "# then calibrate the cameras based on the detections, using iterative bundle adjustment\n",
    "# NOTE: this script will open the simple interface to check the accurate detection of checkerboards\n",
    "# use A or X to accept/reject the boards\n",
    "cgroup.calibrate_videos(vidnames, board)\n",
    "\n",
    "# if you need to save and load\n",
    "# example saving and loading for later\n",
    "cgroup.dump('calibration.toml')\n",
    "\n",
    "## example of loading calibration from a file\n",
    "## you can also load the provided file if you don't want to wait 15 minutes\n",
    "#cgroup = CameraGroup.load('calibration.toml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRIANGULATING THE ACTUAL COORDINATES WITH THE USE OF CALIBRATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from aniposelib.boards import CharucoBoard, Checkerboard\n",
    "from aniposelib.cameras import Camera, CameraGroup\n",
    "from aniposelib.utils import load_pose2d_fnames\n",
    "from coord_correct import correct_coordinate_frame, get_median, ortho, proj\n",
    "\n",
    "#modify the following lines to match your project\n",
    "##################################################\n",
    "output_fname = '//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/10wo timepoint video analysis/Comparing networks/RightHanded/ncb1284-R-reaching7.csv'                     #the file to save the result with 3d trajectory\n",
    "cgroup = CameraGroup.load('//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/10wo timepoint video analysis/Comparing networks/RightHanded/calibration-right.toml')              #the file with calibration from previous step\n",
    "score_threshold = 0.1                                               #lower threshold to accept the point tracked\n",
    "\n",
    "# loading data from DLC\n",
    "fname_dict = {                                                      #name to files you want to triangulate\n",
    "    'A': '//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/10wo timepoint video analysis/Comparing networks/RightHanded/DLC_resnet50_Left_MirrorJun28shuffle1_1000000_filtered_ncb1284-R-camA.h5',\n",
    "    'B': '//pn.vai.org/projects_primary/chu/Hiba Douja Chehade/Behavior/10wo timepoint video analysis/Comparing networks/RightHanded/DLC_resnet50_Center_MirrorJun30shuffle1_990000_filtered_ncb1284-R-camB.h5'\n",
    "}\n",
    "###################################################\n",
    "\n",
    "d = load_pose2d_fnames(fname_dict, cam_names=cgroup.get_names())\n",
    "\n",
    "n_cams, n_points, n_joints, _ = d['points'].shape\n",
    "points = d['points']\n",
    "scores = d['scores']\n",
    "bodyparts = d['bodyparts']\n",
    "\n",
    "# remove points that are below threshold\n",
    "points[scores < score_threshold] = np.nan\n",
    "\n",
    "points_flat = points.reshape(n_cams, -1, 2)\n",
    "scores_flat = scores.reshape(n_cams, -1)\n",
    "\n",
    "p3ds_flat = cgroup.triangulate(points_flat, progress=True)\n",
    "reprojerr_flat = cgroup.reprojection_error(p3ds_flat, points_flat, mean=True)\n",
    "\n",
    "p3ds = p3ds_flat.reshape(n_points, n_joints, 3)\n",
    "reprojerr = reprojerr_flat.reshape(n_points, n_joints)\n",
    "\n",
    "\n",
    "######################################################\n",
    "# This part of the code (if triangulation is set to True) is correcting the coordinate frame to match the axes entered and takes\n",
    "# the reference point as zero, substracting the median of the corresponding column from all points tracked\n",
    "\n",
    "triangulation = True\n",
    "axes = [\n",
    "    [\"x\", \"starting_point\", \"pellet\"],\n",
    "    [\"y\", \"starting_point\", \"border\"]\n",
    "]\n",
    "\n",
    "reference_point = \"pellet\"\n",
    "\n",
    "config=pd.DataFrame(columns=['triangulation'], index=['axes','reference_point'])\n",
    "config['triangulation']['axes'] = [\n",
    "    [\"x\", \"starting_point\", \"pellet\"],\n",
    "    [\"y\", \"starting_point\", \"border\"]\n",
    "]\n",
    "\n",
    "config['triangulation']['reference_point'] = \"pellet\"\n",
    "\n",
    "###\n",
    "\n",
    "if triangulation is True:\n",
    "    all_points_3d_adj, M, center = correct_coordinate_frame(config, p3ds, bodyparts)\n",
    "else:\n",
    "    all_points_3d_adj = p3ds\n",
    "    M = np.identity(3)\n",
    "    center = np.zeros(3)\n",
    "######################################################\n",
    "\n",
    "\n",
    "# concatenating everything in the dataframe and saving to csv   \n",
    "good_points = ~np.isnan(points[:, :, :, 0])\n",
    "num_cams = np.sum(good_points, axis=0).astype('float')\n",
    "scores[~good_points] = 2\n",
    "scores_3d = np.min(scores, axis=0)\n",
    "\n",
    "\n",
    "dout = pd.DataFrame()\n",
    "for bp_num, bp in enumerate(bodyparts):\n",
    "    for ax_num, axis in enumerate(['x','y','z']):\n",
    "        dout[bp + '_' + axis] = all_points_3d_adj[:, bp_num, ax_num]\n",
    "    dout[bp + '_error'] = reprojerr[:, bp_num]\n",
    "    dout[bp + '_ncams'] = num_cams[:, bp_num]\n",
    "    dout[bp + '_score'] = scores_3d[:, bp_num]\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        dout['M_{}{}'.format(i, j)] = M[i, j]\n",
    "\n",
    "for i in range(3):\n",
    "    dout['center_{}'.format(i)] = center[i]\n",
    "\n",
    "dout['fnum'] = np.arange(n_points)\n",
    "\n",
    "dout.to_csv(output_fname, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c6db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASIC VISUAL VERIFICATION OF THE 3D COORDINATES\n",
    "%matplotlib notebook\n",
    "# for more dynamic programs like this one we are switching to the interactive matplotlib backend\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.figure(figsize=(9.4, 6))\n",
    "axes1 = fig.add_subplot(111)\n",
    "axes2 = axes1.twinx()\n",
    "#axes1.plot(p3ds[:, 0, 0])        # you can generate as many plots from p3ds dataframe as you want, for each part triangulated\n",
    "axes1.plot(p3ds[:, 0, 1])\n",
    "#axes2.plot(p3ds[:, 0, 2])\n",
    "plt.xlabel(\"Time (frames)\")\n",
    "plt.ylabel(\"Coordinate (mm)\")\n",
    "plt.title(\"x, y, z coordinates of {}\".format(bodyparts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada222d5",
   "metadata": {},
   "source": [
    "### The end of the first notebook. Proceed to part 2\n",
    "The result of this notebook is the p3ds dataframe with all the coordinates triangulated.\n",
    "It is saved as a csv file in the designated location, so you can open it in the next notebook for\n",
    "automatic/manual analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DEEPLABCUT]",
   "language": "python",
   "name": "conda-env-DEEPLABCUT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
