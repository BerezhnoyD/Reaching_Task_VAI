<h1 id="triangulation-and-analysis-of-behavioral-videos">Triangulation and analysis of behavioral videos</h1>

<p><img src="/images/reconstruction.PNG" alt="drawing" width="400" height="400" /></p>

<p>In addition to the hardware part, we provide the complementary software data analysis pipeline which can be performed off-line with the acquired videos. Python package for the 3d trajectory reconstruction and kinematic analysis is organized in classes and modules implemented in a series of Jupyter Notebooks and based on the state-of-the-art solutions for the markerless pose estimation as well as original code</p>
<ul>
  <li><a href="https://github.com/DeepLabCut/DeepLabCut">DeepLabCut</a> is used for intial tracking using pretrained ResNet architecture (&amp; training it).</li>
  <li><a href="https://github.com/lambdaloop/anipose">Anipose Lib</a> is used for the 3d triangulation part to restore the action in absolute coordinates (mm).</li>
  <li><a href="https://github.com/BerezhnoyD/Reaching_Task_VAI/">ReachOut</a> is our humble code contribution and is used for all the analysis on kinematic traces.</li>
</ul>

<hr />

<h2 id="reachout-kinematic-analysis-pipeline">ReachOut kinematic analysis pipeline</h2>

<p>The <a href="https://github.com/BerezhnoyD/Reaching_Task_VAI/">ReachOut</a> pipeline can be used separately on your own videos and guides the user through the number of steps to extract the parts from the video, track and triangulate the points of interest (paw, fingers), cluster the reaches, visualize and compare basic kinematic parameters for different reaching categories. User has the ability to export the final data on the kinematics (scalars for each reach) and reaching timestamps as a *.h5/.csv table for further analysis or synchronization with his own pipeline.</p>

<p><img src="/images/Slide3.png" alt="alt text" /></p>

<p>Analysis pipeline consists of extracting the parts from the video, 
tracking and triangulating the points of interest (paw, fingers), clustering the reaches, 
visualizing and comparing basic kinematic parameters between different reaching categories.
We suggest copying the whole folder</p>
<ul>
  <li><strong>2D pipeline</strong> or</li>
  <li><strong>3D pipeline</strong></li>
</ul>

<p>from the <a href="https://github.com/BerezhnoyD/Reaching_Task_VAI/tree/main/Analysis%20toolbox">Analysis Toolbox</a>, nd then running the Notebooks using <strong>Jupyter</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="n">cd</span> <span class="n">path</span>\<span class="n">to</span>\<span class="n">the</span> <span class="n">scripts</span>\
<span class="o">&gt;</span> <span class="n">conda</span> <span class="n">activate</span> <span class="n">DEEPLABCUT</span>
<span class="o">&gt;</span> <span class="n">jupyter</span> <span class="n">notebook</span>
</code></pre></div></div>
<hr />

<h2 id="using-reachout-analysis-pipeline">Using ReachOut analysis pipeline</h2>

<p class="note">There are two pipelines provided - one for 3d analysis and reconstruction, and a simpler 2d one, working with only one 
video (single camera view). We mention the differences in the following sections and suggest to start with a simpler one
if your experiment doesn’t specfically require triangulation and representation of coordinates in absolute space (in <em>mm</em>).</p>

<p>The Notebooks are split by their function:
<strong>ReachOut - Tracking</strong> guides the user throught the necessary steps for tracking the points of interest</p>
<ul>
  <li>Cutting the video acquired with FLIR camera into three parts - <em>specific for our setup with two mirrors</em></li>
  <li>Opening the DeepLabCut interface window and running through the full pipeline to track the points of interest</li>
  <li>Updating the csv output from DeepLabCut with absolute position for our reference points</li>
  <li>Using the code from Anipose lib and preprecorded calibration videos to triangulate the points - <em>specific for our setup with two mirrors</em></li>
</ul>

<p><img src="/images/Slide4.png" alt="alt text" /></p>

<p><strong><em>Here are the main steps as described in the Notebooks</em></strong></p>

<hr />

<h3 id="reachout---tracking">ReachOut - Tracking</h3>
<p>**Provides all the scripts to process the behavioral videos and get to the .csv file with **</p>

<p><img src="/images/Notebook1.PNG" alt="alt text" /></p>

<ol>
  <li>Cut the video from the FLIR camera into three parts - three different views of the reach
    <blockquote>
      <p>We are recording the videos of the reaching task using the single high speed widefield FLIR camera and multiple mirrors. 
So the first part is to cut single video into parts corresponding to the Rightside view (CamA), Center view (CamB), Leftside view (CamC) 
with respect to the animal paw. This eliminates the need for synchronisation of different videos, but adds this additional step before we
can analyze our videos with DLC. As a result of this step you should get three video files with the names -camA, -camB, -camC</p>
    </blockquote>
  </li>
  <li>Analyze the video with DeepLabCut
    <blockquote>
      <p>This step is performed using open-source <a href="https://github.com/DeepLabCut/DeepLabCut">DeepLabCut</a> package, the tutorial and more info on which can be found on GitHub.
In short, the first command should launch DeepLabCut interface which guides you through creating the network, training dataset from your videos
(selecting subset of frames and manually labeling the points of interest), training &amp; evaluating the network and finally applying it to all your videos. 
The final output should be a <em>.csv</em> file for each video with coordinates (in pixel dimensions) for all the points tracked by the network. 
We added some code to open, check this table and insert necessary columns with static points for triangulation.</p>
    </blockquote>
  </li>
</ol>

<p class="warning">It should be further noted that despite the Anipose documentation advice to use single DLC network for all views we found that better results
are achieved with two separate networks trained on the side(1) and front(2) view. That means we start DLC analysis for our videos twice - first 
time for all the CamA/C videos, and the second time for CamB. As a result we get two separate sets of <em>*.csv *.h5</em> files. 
For the next step they should have the same name with the difference of last prefix “A,B,C” and copied together to one folder accessible by this Notebook. 
<strong><em>Not needed if using simpler 2D pipeline without triangulation</em></strong></p>

<ol>
  <li>Triangulate the 2d trajectories with Anipose and check the quality of the 3d trajectory
    <blockquote>
      <p>This step is done using the open-source Anipose package, so the tutorial and more info on them can be found on GitHub
Here we provide the code snippet (from <a href="https://github.com/lambdaloop/anipose">Anipose Lib</a>) to calibrate the camera intrinsics using prerecorded videos or checkerboard pattern 
and use the resulting <em>calibration.toml</em> file to triangulate coordinates for the paw in absolute space.</p>

      <p>If you want to use Anipose interface for triangulation you should manually put the resulting .csv and .h5 files into anipose project folder 
<em>*/Anipose project/pose-2d before</em> before starting the triangulation command <em>anipose triangulate</em></p>

      <p>As a result of this step you should get the single <em>*.csv</em> file for each session, containing absolute x,y,z coordinates, that we will use further<br />
<strong><em>Not needed if using simpler 2D pipeline without triangulation</em></strong></p>
    </blockquote>
  </li>
</ol>

<hr />

<h3 id="reachout---analysis">ReachOut - Analysis</h3>
<p><strong>Provides multiple gui snippets to manually segment and annotate the results from DeepLabCut</strong></p>

<p><img src="/images/Notebook2.PNG" alt="alt text" /></p>

<ul>
  <li>Manually scroll through DeepLabCut tracking coordinates, plot them and choose the segments for kinematic analysis (<em>reaches in our case</em>)</li>
  <li>Load the selected parts of the trajectory and manually annotate them from watching the corresponding parts of the video (<em>we defined different reaching categories</em>)</li>
</ul>

<ol>
  <li>Open the <em>.csv</em> file, scroll through the trajectories and manually choose the fragments of the trajectory (<em>with the reaches</em>) for analysis
    <blockquote>
      <p>This script will ask you to choose a file with the OpenFile interface. Point it to the *.csv file generated by the previous notebook. 
It will open the GUI to review the 2d trajectory with time used as a 3rd dimension and extract the fragments of the trajectory for further analysis. 
As a result of this step you will have the *.h5 file with the fragments of the trajectory that you’ve chosen. Open this file to review the trajectories.</p>
    </blockquote>
  </li>
</ol>

<p>Small video showing the workflow</p>

<iframe width="960" height="520" src="/images/Step3.mp4" frameborder="0" allowfullscreen=""></iframe>

<p class="note">In cotrast to the standard 3D pipeline (tracking_split), 2D relies on the different script (tracking_split_2d), all other steps are the same, but with the 
time substituting Z-dimension in the 3D plot, that way you basically see the change in 2d trajectory with time.</p>

<ol>
  <li>Open the h5 file and the corresponding video to review and classify the trajectories
    <blockquote>
      <p>This script is made to load the selected parts of the trajectory and manually annotate them from watching the corresponding parts of the video. 
For this script to work point it to the <em>*.h5</em> file you want to analyze (file) and the video corresponding to the same session (file2.mp4) and then 
use the GUI to rewatch the video fragments and manually assign the labels. As a result of this step you will have the <em>*scalars.h5</em> file which is 
used in the vizualisation steps 5 and 6.</p>
    </blockquote>
  </li>
</ol>

<iframe width="960" height="520" src="/images/Step4.mp4" frameborder="0" allowfullscreen=""></iframe>

<hr />

<h3 id="reachout---visualization">ReachOut - Visualization</h3>
<p><strong>Provides the user with multiple ways to visualize the data and kinematic parameters</strong></p>

<p><img src="/images/Notebook3.PNG" alt="alt text" /></p>

<ul>
  <li>Look at superimposed trajectories for the chosen category of action - <em>reaches in our case</em></li>
  <li>Look at the violin plots for different kinematic parameters for the chosen category of action</li>
  <li>Cluster the actions based on the chosen set of parameters</li>
</ul>

<ol>
  <li>Open the scalars.h5 file and plot all the trajectories of the chosen category
    <blockquote>
      <p>This snippet shows all reaches in the chosen category as Timeseries for visual inspection and analysis. 
From the lists you can choose the category of reaches and the type of data to plot.</p>
    </blockquote>
  </li>
</ol>

<p><img src="/images/Trajectories.PNG" alt="alt text" /></p>

<ol>
  <li>Open the scalars.h5 file and make violin plots for the chosen parameters and reach categories</li>
</ol>

<p><img src="/images/Scalars.PNG" alt="alt text" /></p>

<ol>
  <li>
    <p>Open the scalars.h5 file and automatically cluster the reaches using one of the built in methods</p>
  </li>
  <li>
    <p>Show the number of reaches in each category</p>
  </li>
</ol>

<hr />

<h2 id="qa">Q&amp;A</h2>

<p>If you still have questions after reading this and <a href="https://star-protocols.cell.com/protocols/3539">STAR Protocols</a> please let us
know and we list it in this section.</p>

<hr />

