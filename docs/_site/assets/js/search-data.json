{"0": {
    "doc": "Behavioral apparatus",
    "title": "Behavioral box design",
    "content": "Current protocol is performed with the use of in-house manufactured behavioral box system, which consists of multiple components: . | Plexiglas chamber with a metal bar floor and a slit in the front wall, housing the mouse during the experiment; | Motorized feeder system for precisely delivering and positioning single pellets (5-10 mg Sucrose Tap Pellets, TestDiet) in front of the slit; | Arduino-based controller autonomously performing the programmed experimental protocol (written in C++), interfacing with the touch sensors in the box and providing the master clock signal; | Single FLIR 1.6MP High-Speed camera (Model: Blackfly S BFS-U3-16S2M) with a system of mirrors, saving the video stream on the computer with the use of a Python interface. | . Hence, there are 4 main components that you will need to assemble the behavioral box: . | Plexiglas sheets cut to size and drilled | 3d printed parts for the box such as corners holding the sheets together, | 3d printed feeder and Arduino components | Small screws and nuts kit to put everything together and metal rods for the floor (can be ordered from Amazon) | . All of them can be purchased or manufactured in house with the use of 3d printer and CNC machines or just ordered online. ",
    "url": "/assembly.html#behavioral-box-design",
    
    "relUrl": "/assembly.html#behavioral-box-design"
  },"1": {
    "doc": "Behavioral apparatus",
    "title": "3D printing the parts",
    "content": ".stl files for printing the components are provided in this folder of the repository. Dimensions for the parts and schematics is provided in supplementary materials of the repository and the article. Computer-aided design (CAD) software along with CNC cutter machine can be used to scale and speed up the manufacturing process (all the design files provided were made using FreeCAD software), but the original design can be easily reproduced without it using only all-manual tools. ",
    "url": "/assembly.html#3d-printing-the-parts",
    
    "relUrl": "/assembly.html#3d-printing-the-parts"
  },"2": {
    "doc": "Behavioral apparatus",
    "title": "Q&amp;A",
    "content": "If you still have questions after reading this and the article please let us know and we list it in this section. ",
    "url": "/assembly.html#qa",
    
    "relUrl": "/assembly.html#qa"
  },"3": {
    "doc": "Behavioral apparatus",
    "title": "Behavioral apparatus",
    "content": " ",
    "url": "/assembly.html",
    
    "relUrl": "/assembly.html"
  },"4": {
    "doc": "Hardware part",
    "title": "Hardware overview",
    "content": "Here is the design of the whole setup, everything on the box side is controlled by Arduino, and the experiment can be performed event without the the computer, but you will need the PC to save the streaming data from the camera and the box sensors. The control of the behavioral protocol, all the stimuli and reinforcement (food pellets) presentation, data logging from the sensors and video camera stream activation is handled by the custom circuit, connected to all the components of the reaching box. The schematics are housed in a small 3d printed box and soldered together on a simple breadboard. Most of the components are ready available cicuit modules, which you just need to put together (no extensive knowledge of electronics needed). ",
    "url": "/hardware.html#hardware-overview",
    
    "relUrl": "/hardware.html#hardware-overview"
  },"5": {
    "doc": "Hardware part",
    "title": "Main circuit components",
    "content": ". The same schematics can be found in the Repository and STAR Protocols article, supplementary materials. The main components of the system are the Arduino Nano controller autonomously performing the programmed experimental protocol (written in C++ using Arduino IDE) with different Adafruit shields connected to it. Hence, powered with an external 5V source this behavioral system can be used autonomously for automatic/semi-automatic training of mice to perform the reach-to-grasp task, freeing the experimenter from the manual handling of behavioral protocol, and allowing seamless integration with in vivo electrophysiology recording or other current brain recording/stimulation methods. At the same time, connected to the computer with a USB cable and a simple Python program on the computer side this system saves two data streams and allows live monitoring of the data: basic behavioral data from the touch sensors (position of the animal, paw placement, time spent in front of the slit, timing and number of trials) and 3d view of the reaching movement (from the front camera and two side mirrors) that allows for more sophisticated kinematic analysis of both left and right-hand reaches. ",
    "url": "/hardware.html#main-circuit-components",
    
    "relUrl": "/hardware.html#main-circuit-components"
  },"6": {
    "doc": "Hardware part",
    "title": "Uploading the experimental protocol",
    "content": "The assembled box is programmed with Arduino IDE for its use as a standalone device for automated/semi-automated animal training and behavioral data acquisition. The reaching box can log basic behavioral data (touch/beam sensors) and is designed to be connected to external devices to trigger in vivo imaging, optogenetic stimulation etc. It can stream the synchro signal from the behavioral events recorded by the onboard sensors with millisecond resolution, which makes it ideally fitted for synchronized behavioral-neurophysiology experiments. With the use of multiple mirrors, the system can acquire multiple close-up views for 3d trajectory reconstruction (triangulation) of the reach-to-grasp movement with a single high-speed FLIR camera when connected to the desktop PC. The output of the system is a behavioral video of the reaching movements throughout the learning session along with the .tsv table of the synchronized data from Arduino. After a single upload of the program the board will perform it autonomously on every power up without any actions needed from the user. To switch to another protocol, we’ll need to upload another program using Arduino IDE. The Arduino programming environment can be downloaded from the official website Arduino IDE and is used to write compile and upload the C++ code for Arduino controllers. List of the programs provided in the repository: . | Reaching_Task_Draft – basic protocol with initialization for all the components to explore the structure of the program. | Reaching_Task_Manual - Feeder makes one step clockwise or counterclockwise when the experimenter presses the left or right button respectively. | Reaching_Task_Feeder_Training – Feeder takes one step every 10sec while animal is in the front part of the box. | Reaching_Task_Door_Training – If animal is detected in the front part of the box and grabs the elevated front rod, the feeder takes one step and the door blocking the slit opens (need additional servo motor connected, see Fig.3D). | Reaching_Task_CS_Training - If animal is detected in the front part of the box the speaker delivers 5s beep sound (CS trial) with intertrial intervals of 5s (ITI) and if the animal grabs the elevated front rod during this sound (trial) the feeder makes one step and the door blocking the slit opens | . ",
    "url": "/hardware.html#uploading-the-experimental-protocol",
    
    "relUrl": "/hardware.html#uploading-the-experimental-protocol"
  },"7": {
    "doc": "Hardware part",
    "title": "Q&amp;A",
    "content": "If you still have questions after reading this and STAR Protocols please let us know and we list it in this section. ",
    "url": "/hardware.html#qa",
    
    "relUrl": "/hardware.html#qa"
  },"8": {
    "doc": "Hardware part",
    "title": "Hardware part",
    "content": " ",
    "url": "/hardware.html",
    
    "relUrl": "/hardware.html"
  },"9": {
    "doc": "Home",
    "title": "About the project",
    "content": "ReachOut is maintained © 2022-2024 by Daniil Berezhnoi. All the materials are OpenSource and Authors would be happy to help you with any questions on the code. | The structure of skilled forelimb reaching in the rat: A proximally driven movement with a single distal rotatory component. &#8617; . | Post-stroke kinematic analysis in rats reveals similar reaching abnormalities as humans &#8617; . | Quantitative kinematic characterization of reaching impairments in mice after a stroke &#8617; . | Online control of reach accuracy in mice &#8617; . | ReachingBot: An automated and scalable benchtop device for highly parallel Single Pellet Reach-and-Grasp training and assessment in mice &#8617; . | . ",
    "url": "/#about-the-project",
    
    "relUrl": "/#about-the-project"
  },"10": {
    "doc": "Home",
    "title": "Home",
    "content": "ReachOut project is an open-source hardware-software platform for analyzing reaching movement kinematics in mice. The whole design consists of 3 main parts described here: . | 3D printed box for training mice to perform reach-to-grasp task &amp; acquire behavioral videos for kinematic analysis | Arduino-based controller for executing behavioral protocol &amp; recording animal behavior | ReachOut software (analysis pipeline) to analyze videos &amp; perform kinematic analysis on reconstructed forepaw trajectory | . There is an Article in STAR Protocols with all the details and Repository on GitHub with all materials to recreate the setup . View it in the Article View it on GitHub . This website documents the features of the current main branch accompanying the STARs Protocol article. For the code described in the article use alpha or beta release in the ReachOut repository . Getting Started . Did you know that mice are pretty good at using their forepaws? Well, probably you do and can skip the Science background section and go straight to Project overview or to one of the sections provided in the docs (Navigation on the left sidebar). But how to dissect and study this complex movement? We tried to provide the comprehensive solution to this problem that can be implemented in-lab with a little background knowledge in 3d printing, Arduino and Python Coding. All the installation instructions are currently in the Software part section. You can use the analysis part separately for your project by copying the Jupyter Notebooks. Science background . Reach-to-grasp task in rodents first developed by Ian Whishaw 1 has recently became the gold standard for the study of fine motor behavior in rodents. The structure of the movement and the neurocircuitry behind it can be well translated between rodents and higher primates including humans 23 which makes this task very popular for the study of cortical motor control. There is an increasing use of reach-to-grasp task along current neurophysiological tools like 2-photon imaging or optogenetics for the study of neural activity during execution of the planned motor command 4. However, there are not many designs available for the reach-to-grasp task in freely behaving mice. With the dawn of DIY electronic kits (Arduino) and stereolithography 3D printer technology (OpenCAD) we see the field of open-source behavioral box design rapidly expanding and being a major driver for neuroscience discoveries. There are multiple designs for Skinner boxes and rodent operant chambers proposed by different laboratories. However, there are fewer solutions to study fine motor skills, probably due to the difficulties in precise presentation of the reinforcement and quantifying the kinematics of the small animal movements. The second problem has been recently solved with the use of deep-learning video analysis DeepLabCut. However, the solutions specifically tailored for the behavior-neurophysiology coupling often use head-fixed animals and focused on precise synchronization of the animal behavior, heavily constrained, with the neurophysiology recording. Training animals in such setups can be problematic and labor-intensive. On the other side, apparatuses proposed to study reaching in freely behaving animals, following the line of operant boxes research, usually lack the precision and time resolution needed to synchronize with external devices used in neurophysiology research. Moreover, research with automatic learning boxes, even ReachingBots 5, generally focus on simple metrics like the number of trials and success rate of reaching and overlook more sophisticated kinematic analysis which requires better video quality, precision and post-processing of the behavioral data. Thus, there is always a tight balance between the open-source (ease of manufacture) and research-grade (temporal precision) needed for neurophysiology and precise kinematic studies. Trying to fill this gap we have developed a comprehensive hardware-software setup for mice, using the current state-of-the-art solutions in 3D printing and DNN data analysis and made it accessible to the large open-science community. Project overview . This website describes the basics of design, fabrication, assembly and use case for the open-source hardware platform for mice reach-to-grasp task training and fine motor skill video analysis. You are free to use either the Software part of this project for kinematic analysis (ReachOut pipeline) on the videos from your setup or build the whole behavioral apparatus setup using the instructions provided in the STAR Protocols article. Behavioral apparatus - automated mouse Reaching Box that is 3D printed and hence easily reproducible. The device is used to train animals to perform reach-to-grasp dexterity task: approach the proximal part of the reaching box and reach for single sugar pellets disposed by motorized feeder disk located in front of the narrow slit. This task is used for studying planning and execution of fine motor skills requiring cortical control of the forepaw and finger movement. The semi-automated setup allows to record the video and track the forepaw and fingers during the reaching movement. Hardware used in the design of the task, aside from the high-speed FLIR camera, consists of readily available open-source components that can be acquired relatively cheap or fabricated with the use of stereolithography 3D printer. The ReachOut repository contains all the materials to build the mouse reaching task and analyze the data acquired with this task . | 3D models for the components of the box | Schematics for the reaching box Arduino-based controller | Scripts to run the video recording from the FLIR Camera and logging the data from Arduino | Scripts for the full analysis dataflow: detecting body parts with DLC and getting the reaching trajectories | . Analysis software can be used with the behavioral box to analyze reaching kinematics or as a standalone application for kinematic analysis for the action of your choice (walking, whisker movement, you name it) in your project with certain adaptations. The aim of this part is to simplify keypoint tracking (performed by DLC networks), reviewing tracking results along with the videos and extract different kinematic parameters (speed, acceleration, jerk, accuracy) and different moments of the reach (start, peak, end) to use them as behavioral keypoints (ex. peri-event analysis). We gathered all the code in a series of annotated Jupyter Notebooks for you to use. All the necessary libraries can be installed using provided DEEPLABCUT.yml environment file for Anaconda. For the quick overview and instructions go straight to Software part section. ",
    "url": "/",
    
    "relUrl": "/"
  },"11": {
    "doc": "Software Installation",
    "title": "Installation",
    "content": "Recording toolbox . The recording toolbox contains two parts: . | one on the Arduino side | . Choose one of the provided scripts, edit and upload to the microcontroller with the use of Arduino IDE . | and one on the PC side. This one is just the snippet to record from the FLIR camera and simultaneously record from Arduino. The second part is taken with minor changes from the [PySpin] repository, so you should follow the installation instructions listed there. | . INSTALLATION: Most of the dependencies in the import statements are included in a standard Anaconda installation (i.e. PIL, Numpy, Tkinter) and with your NVIDIA graphics card (i.e CUDA) for the GPU versions. PySpin and the Spinnaker API must be downloaded from the FLIR website (https://www.flir.com/products/spinnaker-sdk/); choose the appropriate version of Spinnaker and install first, then install the “Latest Python Spinnaker” version that is compatible with your version of Python (I’ve tested with Python 3.5 &amp; 3.8) using the instructions in the ReadMe file. An FFMPEG executable needs to be downlaoded (https://ffmpeg.org/download.html) and placed in a folder that you can point to in the import statements, such as within the site-packages folder of your Python installation. Finally, scikit-video (http://www.scikit-video.org/stable/) can be added to your Python installation using pip. After installing PySpinCapture simply run one of the scripts provided in the [ReachOut] repository [Recording Toolbox] like that: . &gt; cd path\\to\\the repository\\ &gt; python CameraRoll.py . Analysis toolbox . Analysis toolbox included with this project can be used as a part of the proposed [STARs Protocols] pipeline or as a standalone application The analysis toolbox is dependent on certain python libraries and programs (DEEPLABCUT, ANIPOSE) and Jupyter Notebook. To make it easier for installation all the dependencies are included in the updated *DEEPLABCUT.yml conda environment file contained in [ReachOut] repository [Analysis Toolbox]. After installing Anaconda just run the following commands: . &gt; cd path\\to\\the repository\\Analysis toolbox\\ &gt; conda env create -f DEEPLABCUT.yml . ",
    "url": "/install.html#installation",
    
    "relUrl": "/install.html#installation"
  },"12": {
    "doc": "Software Installation",
    "title": "Q&amp;A",
    "content": "If you still have questions after reading this and [STAR Protocols] please let us know and we list it in this section. If you experience any bugs during the installation or use please let us know ASAP so we can fix them in the next release. We appreciate your feedback . ",
    "url": "/install.html#qa",
    
    "relUrl": "/install.html#qa"
  },"13": {
    "doc": "Software Installation",
    "title": "Software Installation",
    "content": " ",
    "url": "/install.html",
    
    "relUrl": "/install.html"
  },"14": {
    "doc": "Software part",
    "title": "Triangulation and analysis of behavioral videos",
    "content": ". In addition to the hardware part, we provide the complementary software data analysis pipeline which can be performed off-line with the acquired videos. Python package for the 3d trajectory reconstruction and kinematic analysis is organized in classes and modules implemented in a series of Jupyter Notebooks and based on the state-of-the-art solutions for the markerless pose estimation as well as original code . | DeepLabCut is used for intial tracking using pretrained ResNet architecture (&amp; training it). | Anipose Lib is used for the 3d triangulation part to restore the action in absolute coordinates (mm). | ReachOut is our humble code contribution and is used for all the analysis on kinematic traces. | . ",
    "url": "/software.html#triangulation-and-analysis-of-behavioral-videos",
    
    "relUrl": "/software.html#triangulation-and-analysis-of-behavioral-videos"
  },"15": {
    "doc": "Software part",
    "title": "ReachOut kinematic analysis pipeline",
    "content": "The ReachOut pipeline can be used separately on your own videos and guides the user through the number of steps to extract the parts from the video, track and triangulate the points of interest (paw, fingers), cluster the reaches, visualize and compare basic kinematic parameters for different reaching categories. User has the ability to export the final data on the kinematics (scalars for each reach) and reaching timestamps as a *.h5/.csv table for further analysis or synchronization with his own pipeline. Analysis pipeline consists of extracting the parts from the video, tracking and triangulating the points of interest (paw, fingers), clustering the reaches, visualizing and comparing basic kinematic parameters between different reaching categories. We suggest copying the whole folder . | 2D pipeline or | 3D pipeline | . from the Analysis Toolbox, nd then running the Notebooks using Jupyter . &gt; cd path\\to\\the scripts\\ &gt; conda activate DEEPLABCUT &gt; jupyter notebook . ",
    "url": "/software.html#reachout-kinematic-analysis-pipeline",
    
    "relUrl": "/software.html#reachout-kinematic-analysis-pipeline"
  },"16": {
    "doc": "Software part",
    "title": "Using ReachOut analysis pipeline",
    "content": "There are two pipelines provided - one for 3d analysis and reconstruction, and a simpler 2d one, working with only one video (single camera view). We mention the differences in the following sections and suggest to start with a simpler one if your experiment doesn’t specfically require triangulation and representation of coordinates in absolute space (in mm). The Notebooks are split by their function: ReachOut - Tracking guides the user throught the necessary steps for tracking the points of interest . | Cutting the video acquired with FLIR camera into three parts - specific for our setup with two mirrors | Opening the DeepLabCut interface window and running through the full pipeline to track the points of interest | Updating the csv output from DeepLabCut with absolute position for our reference points | Using the code from Anipose lib and preprecorded calibration videos to triangulate the points - specific for our setup with two mirrors | . Here are the main steps as described in the Notebooks . ReachOut - Tracking . **Provides all the scripts to process the behavioral videos and get to the .csv file with ** . | Cut the video from the FLIR camera into three parts - three different views of the reach We are recording the videos of the reaching task using the single high speed widefield FLIR camera and multiple mirrors. So the first part is to cut single video into parts corresponding to the Rightside view (CamA), Center view (CamB), Leftside view (CamC) with respect to the animal paw. This eliminates the need for synchronisation of different videos, but adds this additional step before we can analyze our videos with DLC. As a result of this step you should get three video files with the names -camA, -camB, -camC . | Analyze the video with DeepLabCut This step is performed using open-source DeepLabCut package, the tutorial and more info on which can be found on GitHub. In short, the first command should launch DeepLabCut interface which guides you through creating the network, training dataset from your videos (selecting subset of frames and manually labeling the points of interest), training &amp; evaluating the network and finally applying it to all your videos. The final output should be a .csv file for each video with coordinates (in pixel dimensions) for all the points tracked by the network. We added some code to open, check this table and insert necessary columns with static points for triangulation. | . It should be further noted that despite the Anipose documentation advice to use single DLC network for all views we found that better results are achieved with two separate networks trained on the side(1) and front(2) view. That means we start DLC analysis for our videos twice - first time for all the CamA/C videos, and the second time for CamB. As a result we get two separate sets of *.csv *.h5 files. For the next step they should have the same name with the difference of last prefix “A,B,C” and copied together to one folder accessible by this Notebook. Not needed if using simpler 2D pipeline without triangulation . | Triangulate the 2d trajectories with Anipose and check the quality of the 3d trajectory This step is done using the open-source Anipose package, so the tutorial and more info on them can be found on GitHub Here we provide the code snippet (from Anipose Lib) to calibrate the camera intrinsics using prerecorded videos or checkerboard pattern and use the resulting calibration.toml file to triangulate coordinates for the paw in absolute space. If you want to use Anipose interface for triangulation you should manually put the resulting .csv and .h5 files into anipose project folder */Anipose project/pose-2d before before starting the triangulation command anipose triangulate . As a result of this step you should get the single *.csv file for each session, containing absolute x,y,z coordinates, that we will use further Not needed if using simpler 2D pipeline without triangulation . | . ReachOut - Analysis . Provides multiple gui snippets to manually segment and annotate the results from DeepLabCut . | Manually scroll through DeepLabCut tracking coordinates, plot them and choose the segments for kinematic analysis (reaches in our case) | Load the selected parts of the trajectory and manually annotate them from watching the corresponding parts of the video (we defined different reaching categories) | . | Open the .csv file, scroll through the trajectories and manually choose the fragments of the trajectory (with the reaches) for analysis This script will ask you to choose a file with the OpenFile interface. Point it to the *.csv file generated by the previous notebook. It will open the GUI to review the 2d trajectory with time used as a 3rd dimension and extract the fragments of the trajectory for further analysis. As a result of this step you will have the *.h5 file with the fragments of the trajectory that you’ve chosen. Open this file to review the trajectories. | . Small video showing the workflow . In cotrast to the standard 3D pipeline (tracking_split), 2D relies on the different script (tracking_split_2d), all other steps are the same, but with the time substituting Z-dimension in the 3D plot, that way you basically see the change in 2d trajectory with time. | Open the h5 file and the corresponding video to review and classify the trajectories This script is made to load the selected parts of the trajectory and manually annotate them from watching the corresponding parts of the video. For this script to work point it to the *.h5 file you want to analyze (file) and the video corresponding to the same session (file2.mp4) and then use the GUI to rewatch the video fragments and manually assign the labels. As a result of this step you will have the *scalars.h5 file which is used in the vizualisation steps 5 and 6. | . ReachOut - Visualization . Provides the user with multiple ways to visualize the data and kinematic parameters . | Look at superimposed trajectories for the chosen category of action - reaches in our case | Look at the violin plots for different kinematic parameters for the chosen category of action | Cluster the actions based on the chosen set of parameters | . | Open the scalars.h5 file and plot all the trajectories of the chosen category This snippet shows all reaches in the chosen category as Timeseries for visual inspection and analysis. From the lists you can choose the category of reaches and the type of data to plot. | . | Open the scalars.h5 file and make violin plots for the chosen parameters and reach categories | . | Open the scalars.h5 file and automatically cluster the reaches using one of the built in methods . | Show the number of reaches in each category . | . ",
    "url": "/software.html#using-reachout-analysis-pipeline",
    
    "relUrl": "/software.html#using-reachout-analysis-pipeline"
  },"17": {
    "doc": "Software part",
    "title": "Q&amp;A",
    "content": "If you still have questions after reading this and STAR Protocols please let us know and we list it in this section. ",
    "url": "/software.html#qa",
    
    "relUrl": "/software.html#qa"
  },"18": {
    "doc": "Software part",
    "title": "Software part",
    "content": " ",
    "url": "/software.html",
    
    "relUrl": "/software.html"
  }
}
